{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d4980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code that tunes and trains the ARIMA model on all the exchange rates to perform forecasts and calculate the model uncertainty using the GARCH model.\n",
    "# Author Samwel Portelli <samwel.portelli.18@um.edu.mt>\n",
    "# Code to tune and update the ARIMA model adapted from http://alkaline-ml.com/2019-12-18-pmdarima-1-5-2/ (last accessed: 28/11/2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88eaa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deel.puncc.metrics import regression_ace\n",
    "from deel.puncc.metrics import regression_mean_coverage\n",
    "from deel.puncc.metrics import regression_sharpness\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pmdarima\n",
    "import arch\n",
    "import scipy.stats as stats\n",
    "import csv\n",
    "from pmdarima.arima import ndiffs\n",
    "import pmdarima as pm\n",
    "import warnings; \n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import csv\n",
    "\n",
    "def direction_accuracy(actual, predicted):\n",
    "    \n",
    "    # Ensure the lengths of actual and predicted are the same\n",
    "    if len(actual) != len(predicted):\n",
    "        raise ValueError(\"Input lists must have the same length.\")\n",
    "\n",
    "    # Calculate differences between consecutive values\n",
    "    actual_diff = [actual[i] - actual[i-1] for i in range(1, len(actual))]\n",
    "    predicted_diff = [predicted[i] - actual[i-1] for i in range(1, len(predicted))]\n",
    "\n",
    "    # Count the number of correct predictions\n",
    "    correct_predictions = sum((a * p) > 0 for a, p in zip(actual_diff, predicted_diff))\n",
    "\n",
    "    # Calculate accuracy as a percentage\n",
    "    accuracy_percentage = (correct_predictions / len(actual_diff)) * 100.0\n",
    "\n",
    "    return float(accuracy_percentage)\n",
    "\n",
    "def create_unique_folder(folder_name):\n",
    "    version = 1\n",
    "    new_folder_name = folder_name\n",
    "    while os.path.exists(new_folder_name):\n",
    "        new_folder_name = f\"{folder_name}_v{version}\"\n",
    "        version += 1\n",
    "    os.makedirs(new_folder_name)\n",
    "    return new_folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beef1209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_one_step(exog_y_test_values, alpha):\n",
    "    fc, conf_int = model.predict(n_periods=1,X=exog_y_test_values, return_conf_int=True, alpha=alpha)\n",
    "    return (\n",
    "        fc.tolist()[0],\n",
    "        np.asarray(conf_int).tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6442760b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the exchange_rate dataset\n",
    "data = pd.read_csv(\"C:/Users/porte/Downloads/dataset/exchange_rate/exchange_rate.csv\", index_col='date', parse_dates=True)\n",
    "\n",
    "# Select the relevant columns for the model\n",
    "columns = ['0', '1', '2', '3', '4', '5', '6', 'OT']\n",
    "\n",
    "window_size = 96\n",
    "\n",
    "train_data = data.iloc[:5311]\n",
    "test_data = data.iloc[5215:]\n",
    "\n",
    "train_end = 5311\n",
    "test_start = 5311\n",
    "lags = 8\n",
    "\n",
    "alphas=[0.01, 0.05, 0.1, 0.15]\n",
    "\n",
    "confidence=np.subtract(np.ones(len(alphas)), alphas)\n",
    "\n",
    "targets_to_run = ['0', '1', '2', '3', '4', '5', '6', 'OT']\n",
    "\n",
    "folder_names = ['.\\\\arima_garch_8lags\\\\val_arima_garch_model_data_0', '.\\\\arima_garch_9lags\\\\val_arima_garch_model_data_1',  '.\\\\arima_garch_9lags\\\\val_arima_garch_model_data_2',  '.\\\\arima_garch_9lags\\\\val_arima_garch_model_data_3', '.\\\\arima_garch_9lags\\\\val_arima_garch_model_data_4', '.\\\\arima_garch_9lags\\\\val_arima_garch_model_data_5', '.\\\\arima_garch_9lags\\\\val_arima_garch_model_data_6', '.\\\\arima_garch_9lags\\\\val_arima_garch_model_data_OT']\n",
    "test_folder_names = ['.\\\\arima_garch_9lags\\\\test_arima_garch_model_data_0', '.\\\\arima_garch_9lags\\\\test_arima_garch_model_data_1', '.\\\\arima_garch_9lags\\\\test_arima_garch_model_data_2', '.\\\\arima_garch_9lags\\\\test_arima_garch_model_data_3', '.\\\\arima_garch_9lags\\\\test_arima_garch_model_data_4', '.\\\\arima_garch_9lags\\\\test_arima_garch_model_data_5',  '.\\\\arima_garch_9lags\\\\test_arima_garch_model_data_6', '.\\\\arima_garch_9lags\\\\test_arima_garch_model_data_OT']\n",
    "\n",
    "orders = []\n",
    "model_alpha = []\n",
    "\n",
    "for ind, target in enumerate(targets_to_run):\n",
    "     \n",
    "    folder_name = create_unique_folder(folder_names[ind])\n",
    "    test_folder_name = create_unique_folder(test_folder_names[ind])\n",
    "    \n",
    "    # Create rolling windows for training data\n",
    "    train_X, train_y = create_rolling_windows(train_data, window_size, target)\n",
    "\n",
    "    # Create rolling windows for testing data\n",
    "    test_X, test_y = create_rolling_windows(test_data, window_size, target)\n",
    "\n",
    "    ace_list, mean_coverage_list, average_width_list, direction_accuracy_value_list = [], [], [], []\n",
    "\n",
    "    for alpha in alphas:\n",
    "        \n",
    "        # Reading and preparing the data\n",
    "        df = pd.read_csv(\"C:/Users/porte/Downloads/dataset/exchange_rate/exchange_rate.csv\", usecols=[target], parse_dates=True)\n",
    "        df.rename(columns={target: \"Open\"}, inplace=True)\n",
    "\n",
    "        exog_var = targets.remove(target)\n",
    "\n",
    "        exog_df = pd.read_csv(\"C:/Users/porte/Downloads/dataset/exchange_rate/exchange_rate.csv\", usecols=exog_var, parse_dates=False)\n",
    "        exog_df = exog_df.drop(['date'], axis=1)\n",
    "\n",
    "        y_train = df[1:train_end].values\n",
    "        y_test = df[test_start:].values\n",
    "\n",
    "        exog_y_train = exog_df[:train_end-1].values\n",
    "        exog_y_test = exog_df[test_start:].values\n",
    "        exog_y_test = np.concatenate((np.array([exog_y_train[-1]]), exog_y_test[:-1]))\n",
    "        \n",
    "        # Finding the best parameters for the model\n",
    "        kpss_diffs = ndiffs(y_train, alpha=0.05, test='kpss', max_d=6)\n",
    "        adf_diffs = ndiffs(y_train, alpha=0.05, test='adf', max_d=6)\n",
    "        n_diffs = max(adf_diffs, kpss_diffs)\n",
    "\n",
    "        # Training the arima model\n",
    "        model = pm.auto_arima(y_train, exog_y_train, d=n_diffs, seasonal=False, stepwise=True,\n",
    "                         suppress_warnings=True, max_p=6, trace=2)\n",
    "\n",
    "        print(model.order)\n",
    "        orders.append(model.order)\n",
    "        model_alpha.append(str(target)+'_'+str(alpha))\n",
    "\n",
    "        arima_residuals = model.resid()\n",
    "        arima_residuals = arima_residuals[-lags:]\n",
    "\n",
    "        print('Testing alpha value: '+str(alpha))\n",
    "        \n",
    "        # Fitting the garch to the residuals of the arima model\n",
    "        garch = arch.arch_model(arima_residuals, p=1, q=1, vol=\"Garch\", dist=\"Normal\")\n",
    "        garch_fitted = garch.fit(update_freq=1)\n",
    "\n",
    "        # Generate forecasts for the next 1422 periods (testing set)\n",
    "        forecast_values, forecast_intervals_lower, forecast_intervals_upper, garch_upper, garch_lower = [], [], [], [], []\n",
    "        \n",
    "        for ind, new_ob in enumerate(y_test):\n",
    "            \n",
    "            fc, conf = forecast_one_step(np.array([exog_y_test[ind]]), alpha)\n",
    "            forecast_values.append(fc)\n",
    "            forecast_intervals_lower.append(conf[0])\n",
    "            forecast_intervals_upper.append(conf[1])\n",
    "\n",
    "            # Updates the existing model with a small number of MLE steps\n",
    "            model.update(new_ob, np.array([exog_y_test[ind]]))\n",
    "            \n",
    "            # Obtaining the prediction intervals\n",
    "            garch_forecast = garch_fitted.forecast(horizon=1)\n",
    "            predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "            garch_variance = garch_forecast.residual_variance['h.1'].iloc[-1]\n",
    "\n",
    "            garch_upper.append(forecast_values[-1] + stats.norm.ppf(1 - alpha / 2)*np.sqrt(garch_variance))\n",
    "            garch_lower.append(forecast_values[-1] - stats.norm.ppf(1 - alpha / 2)*np.sqrt(garch_variance))\n",
    "\n",
    "            if ind==0:\n",
    "                updated_errors = np.concatenate((arima_residuals[1:],(y_test[ind]-fc)))\n",
    "            else:\n",
    "                updated_errors = np.concatenate((updated_errors[1:],(y_test[ind]-fc)))\n",
    "\n",
    "            # re-fit GARCH model\n",
    "            garch = arch.arch_model(updated_errors, p=1, q=1, vol=\"Garch\", dist=\"Normal\")\n",
    "            garch_fitted = garch.fit(update_freq=0, disp=False)\n",
    "            \n",
    "        valpreds = forecast_values[:760]\n",
    "        testpreds = forecast_values[760:]\n",
    "        \n",
    "        valtrues = test_y[:760]\n",
    "        testtrues = test_y[760:]\n",
    "        \n",
    "        val_garch_upper = garch_upper[:760]\n",
    "        val_garch_lower = garch_lower[:760]\n",
    "        test_garch_upper = garch_upper[760:]\n",
    "        test_garch_lower = garch_lower[760:]\n",
    "        \n",
    "        # VALIDATION\n",
    "        forecast_intervals_lower = np.array(val_garch_lower).flatten()\n",
    "        forecast_intervals_upper = np.array(val_garch_upper).flatten()\n",
    "\n",
    "\n",
    "        # Create a dictionary with the arrays\n",
    "        actual_pred_data = {\n",
    "            'y_true': valtrues,\n",
    "            'y_pred_from_arimagarch': np.array(valpreds).flatten(),\n",
    "            'y_lower': forecast_intervals_lower,\n",
    "            'y_upper': forecast_intervals_upper\n",
    "        }\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(actual_pred_data)\n",
    "\n",
    "        # Specify the CSV file name\n",
    "        preds_actual_csv_file_name = folder_name  + '\\\\val_arimagarch_y_true_and_pred_alp_'+str(int((1-alpha)*100))+'.csv'\n",
    "\n",
    "        # Write to the CSV file\n",
    "        df.to_csv(preds_actual_csv_file_name, index=False)\n",
    "\n",
    "        print(f\"Data saved to {preds_actual_csv_file_name}\")\n",
    "        \n",
    "        # TEST\n",
    "        forecast_intervals_lower = np.array(test_garch_lower).flatten()\n",
    "        forecast_intervals_upper = np.array(test_garch_upper).flatten()\n",
    "\n",
    "        # Create a dictionary with the arrays\n",
    "        actual_pred_data = {\n",
    "            'y_true': testtrues,\n",
    "            'y_pred_from_arimagarch': np.array(testpreds).flatten(),\n",
    "            'y_lower': forecast_intervals_lower,\n",
    "            'y_upper': forecast_intervals_upper\n",
    "        }\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(actual_pred_data)\n",
    "\n",
    "        # Specify the CSV file name\n",
    "        preds_actual_csv_file_name = test_folder_name  + '\\\\arimagarch_y_true_and_pred_alp_'+str(int((1-alpha)*100))+'.csv'\n",
    "\n",
    "        # Write to the CSV file\n",
    "        df.to_csv(preds_actual_csv_file_name, index=False)\n",
    "\n",
    "        print(f\"Data saved to {preds_actual_csv_file_name}\")\n",
    "\n",
    "    # Combine lists into a list of tuples\n",
    "    metric_data = list(zip(ace_list, mean_coverage_list, average_width_list, direction_accuracy_value_list))\n",
    "\n",
    "    # Specify the CSV file name\n",
    "    csv_file_name = folder_name + '\\\\val_arimagarch_multiple_alpha_metrics_data.csv'\n",
    "\n",
    "    # Write to the CSV file\n",
    "    with open(csv_file_name, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the header\n",
    "        writer.writerow(['ACE', 'Mean Coverage', 'Average Width', 'Direction Accuracy'])\n",
    "        # Write the data\n",
    "        writer.writerows(metric_data)\n",
    "\n",
    "    print(f\"Metrics saved to {csv_file_name}\")\n",
    "    \n",
    "# Open a CSV file for writing\n",
    "with open('.\\\\arima_garch_8lags\\\\arima_orders_'+target+'.csv', 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    \n",
    "    # Write the header\n",
    "    csvwriter.writerow(['target_alpha', 'order'])\n",
    "    \n",
    "    # Write the data\n",
    "    for string, tuple_value in zip(model_alpha, orders):\n",
    "        csvwriter.writerow([string, tuple_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49a5564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
