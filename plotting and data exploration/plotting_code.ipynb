{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec2b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code used for plotting data\n",
    "# Author Samwel Portelli <samwel.portelli.18@um.edu.mt>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acdaa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deel.puncc.metrics import regression_ace\n",
    "from deel.puncc.metrics import regression_mean_coverage\n",
    "from deel.puncc.metrics import regression_sharpness\n",
    "from deel.puncc.plotting import plot_prediction_intervals\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951ca7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['arima', 'arima_garch', 'transformer_mcdropout', 'transformer_garch', 'transformer_enbpi']\n",
    "alphas = [0.15, 0.1, 0.05, 0.01]\n",
    "\n",
    "data_folders = ['\\model_data_1','\\model_data_2','\\model_data_3','\\model_data_5','\\model_data_6','\\model_data_OT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c270a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(model, alpha, s=None, data_folder='\\model_data_OT'):\n",
    "    \n",
    "    folder_path = r'C:\\Users\\porte\\Downloads' + data_folder\n",
    "    \n",
    "    if model=='transformer_enbpi':\n",
    "        \n",
    "        file_path = os.path.join(folder_path, f'transformer_confidence_run_data_s_{s}', f'ALL_PI_data_s_{s}.csv')\n",
    "        file_path_true_pred = os.path.join(folder_path, f'transformer_confidence_run_data_s_{s}', f'y_true_and_pred_s_{s}.csv')\n",
    "        og_data = r'C:\\Users\\porte\\Downloads\\dataset\\exchange_rate\\exchange_rate.csv'\n",
    "        val_file_path = os.path.join(folder_path, f'transformer_confidence_run_data_s_{s}_val', f'ALL_PI_data_s_{s}.csv')\n",
    "        val_file_path_true_pred = os.path.join(folder_path, f'transformer_confidence_run_data_s_{s}_val', f'y_true_and_pred_s_{s}.csv')\n",
    "    \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path, delimiter=',')\n",
    "\n",
    "        # Iterate through each column and convert it to an array\n",
    "        confidence_data = {}\n",
    "        for column in df.columns:\n",
    "            confidence_data[column] = df[column].to_numpy()\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path_true_pred, delimiter=',')\n",
    "\n",
    "        # Iterate through each column and convert it to an array\n",
    "        true_pred = {}\n",
    "        for column in df.columns:\n",
    "            true_pred[column] = df[column].to_numpy()\n",
    "\n",
    "        # VALIDATION DATA\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(val_file_path, delimiter=',')\n",
    "\n",
    "        # Iterate through each column and convert it to an array\n",
    "        val_confidence_data = {}\n",
    "        for column in df.columns:\n",
    "            val_confidence_data[column] = df[column].to_numpy()\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(val_file_path_true_pred, delimiter=',')\n",
    "\n",
    "        # Iterate through each column and convert it to an array\n",
    "        val_true_pred = {}\n",
    "        for column in df.columns:\n",
    "            val_true_pred[column] = df[column].to_numpy()\n",
    "\n",
    "        # ORIGINAL DATA\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(og_data, delimiter=',')\n",
    "\n",
    "        # Iterate through each column and convert it to an array\n",
    "        og_data = {}\n",
    "        for column in df.columns:\n",
    "            og_data[column] = df[column].to_numpy()\n",
    "\n",
    "        #Test Set\n",
    "        y_true=true_pred['y_true']\n",
    "        y_pred=true_pred['y_pred_from_PUNCC']\n",
    "        y_pred_lower=confidence_data['low_'+str(alpha)]\n",
    "        y_pred_upper=confidence_data['high_'+str(alpha)]\n",
    "\n",
    "        # Validation Set\n",
    "        val_y_true=val_true_pred['y_true']\n",
    "        val_y_pred=val_true_pred['y_pred_from_PUNCC']\n",
    "        val_y_pred_lower=val_confidence_data['low_'+str(alpha)]\n",
    "        val_y_pred_upper=val_confidence_data['high_'+str(alpha)]\n",
    "\n",
    "        og_data_true=og_data[data_folder.split('_')[-1]]\n",
    "        og_data_date=og_data['date']\n",
    "    \n",
    "    elif model=='transformer_garch':\n",
    "        conf = int(100-alpha*100)\n",
    "        \n",
    "        file_path = os.path.join(folder_path, f'garch_transformer_confidence_run_data_alpha_{alpha}', f'garch_transforemer_y_true_and_pred_alp_{conf}_{data_folder.split(\"_\")[-1]}.csv')\n",
    "        #file_path = r'C:\\Users\\porte\\Downloads'+str(data_folder)+'\\\\garch_transformer_confidence_run_data_alpha_'+str(alpha)+'\\garch_transforemer_y_true_and_pred_alp_'+str(conf)+'.csv'\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        y_true = df['y_true'].values\n",
    "        y_pred = df['y_pred_from_PUNCC'].values\n",
    "        y_pred_lower = df['y_lower'].values\n",
    "        y_pred_upper = df['y_upper'].values\n",
    "\n",
    "        file_path = os.path.join(folder_path, f'val_garch_transformer_confidence_run_data_alpha_{alpha}', f'val_garch_transforemer_y_true_and_pred_alp_{conf}_{data_folder.split(\"_\")[-1]}.csv')\n",
    "        #file_path = r'C:\\Users\\porte\\Downloads'+str(data_folder)+'\\\\val_garch_transformer_confidence_run_data_alpha_'+str(alpha)+'\\garch_transforemer_y_true_and_pred_alp_'+str(conf)+'.csv'\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        val_y_true = df['y_true'].values\n",
    "        val_y_pred = df['y_pred_from_PUNCC'].values\n",
    "        val_y_pred_lower = df['y_lower'].values\n",
    "        val_y_pred_upper = df['y_upper'].values\n",
    "\n",
    "        og_data = r'C:\\Users\\porte\\Downloads\\dataset\\exchange_rate\\exchange_rate.csv'\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(og_data, delimiter=',')\n",
    "\n",
    "        # Iterate through each column and convert it to an array\n",
    "        og_data = {}\n",
    "        for column in df.columns:\n",
    "            og_data[column] = df[column].to_numpy()\n",
    "\n",
    "        og_data_true=og_data[data_folder.split('_')[-1]]\n",
    "        og_data_date=og_data['date'] \n",
    "        \n",
    "    elif model=='transformer_mcdropout':\n",
    "        conf = int(100-alpha*100)\n",
    "\n",
    "        file_path = os.path.join(folder_path, 'output_mc_1_MCDROPOUT_PREDICTION', 'output_mc_1',f'min_max_mean_alpha_{alpha}.csv')\n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        file_path = os.path.join(folder_path, 'output_mc_1_MCDROPOUT_PREDICTION', 'output_mc_1','true_0.npy')\n",
    "        \n",
    "        y_true = np.load(file_path).flatten()\n",
    "        y_pred = df['mean'].values\n",
    "        y_pred_lower = df['lower'].values\n",
    "        y_pred_upper = df['upper'].values\n",
    "\n",
    "\n",
    "        file_path = os.path.join(folder_path, 'output_mc_1_MCDROPOUT_VAL', 'output_mc_1',f'min_max_mean_alpha_{alpha}.csv')\n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        file_path = os.path.join(folder_path, 'output_mc_1_MCDROPOUT_VAL', 'output_mc_1','true_0.npy')\n",
    "        \n",
    "        val_y_true = np.load(file_path).flatten()\n",
    "        \n",
    "        val_y_pred = df['mean'].values\n",
    "        val_y_pred_lower = df['lower'].values\n",
    "        val_y_pred_upper = df['upper'].values\n",
    "\n",
    "        og_data = r'C:\\Users\\porte\\Downloads\\dataset\\exchange_rate\\exchange_rate.csv'\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(og_data, delimiter=',')\n",
    "\n",
    "        # Iterate through each column and convert it to an array\n",
    "        og_data = {}\n",
    "        for column in df.columns:\n",
    "            og_data[column] = df[column].to_numpy()\n",
    "\n",
    "        og_data_true=og_data[data_folder.split('_')[-1]]\n",
    "        og_data_date=og_data['date']        \n",
    "    elif model=='arima_garch':\n",
    "        conf = int(100-alpha*100)\n",
    "\n",
    "        data_file_path = r'C:\\Users\\porte\\Downloads'+str(data_folder)+'\\\\arimagarch_y_true_and_pred_alp_'+str(conf)+'.csv'\n",
    "\n",
    "        # Read data CSV file\n",
    "        df_data = pd.read_csv(data_file_path, delimiter=',')\n",
    "\n",
    "        # Store columns in arrays\n",
    "        y_true = df_data['y_true'].values\n",
    "        y_pred = df_data['y_pred_from_arimagarch'].values \n",
    "        y_pred_lower = df_data['y_lower'].values\n",
    "        y_pred_upper = df_data['y_upper'].values\n",
    "\n",
    "        data_file_path = r'C:\\Users\\porte\\Downloads'+str(data_folder)+'\\\\val_arimagarch_y_true_and_pred_alp_'+str(conf)+'.csv'\n",
    "\n",
    "        # Read data CSV file\n",
    "        df_data = pd.read_csv(data_file_path, delimiter=',')\n",
    "\n",
    "        # Store columns in arrays\n",
    "        val_y_true = df_data['y_true'].values\n",
    "        val_y_pred = df_data['y_pred_from_arimagarch'].values \n",
    "        val_y_pred_lower = df_data['y_lower'].values\n",
    "        val_y_pred_upper = df_data['y_upper'].values\n",
    "\n",
    "        og_data = r'C:\\Users\\porte\\Downloads\\dataset\\exchange_rate\\exchange_rate.csv'\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(og_data, delimiter=',')\n",
    "\n",
    "        # Iterate through each column and convert it to an array\n",
    "        og_data = {}\n",
    "        for column in df.columns:\n",
    "            og_data[column] = df[column].to_numpy()\n",
    "\n",
    "        og_data_true=og_data[data_folder.split('_')[-1]]\n",
    "        og_data_date=og_data['date']        \n",
    "    elif model=='arima':\n",
    "        conf = int(100-alpha*100)\n",
    "\n",
    "        data_file_path = r'C:\\Users\\porte\\Downloads'+str(data_folder)+'\\\\arima_y_true_and_pred_alp_'+str(conf)+'.csv'\n",
    "\n",
    "        # Read data CSV file\n",
    "        df_data = pd.read_csv(data_file_path, delimiter=',')\n",
    "\n",
    "        # Store columns in arrays\n",
    "        y_true = df_data['y_true'].values\n",
    "        y_pred = df_data['y_pred_from_arima'].values \n",
    "        y_pred_lower = df_data['y_lower'].values\n",
    "        y_pred_upper = df_data['y_upper'].values\n",
    "\n",
    "        data_file_path = r'C:\\Users\\porte\\Downloads'+str(data_folder)+'\\\\val_arima_y_true_and_pred_alp_'+str(conf)+'.csv'\n",
    "\n",
    "        # Read data CSV file\n",
    "        df_data = pd.read_csv(data_file_path, delimiter=',')\n",
    "\n",
    "        # Store columns in arrays\n",
    "        val_y_true = df_data['y_true'].values\n",
    "        val_y_pred = df_data['y_pred_from_arima'].values \n",
    "        val_y_pred_lower = df_data['y_lower'].values\n",
    "        val_y_pred_upper = df_data['y_upper'].values\n",
    "\n",
    "        og_data = r'C:\\Users\\porte\\Downloads\\dataset\\exchange_rate\\exchange_rate.csv'\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(og_data, delimiter=',')\n",
    "\n",
    "        # Iterate through each column and convert it to an array\n",
    "        og_data = {}\n",
    "        for column in df.columns:\n",
    "            og_data[column] = df[column].to_numpy()\n",
    "\n",
    "        og_data_true=og_data[data_folder.split('_')[-1]]\n",
    "        og_data_date=og_data['date']        \n",
    "    else:\n",
    "        print('Model does not exist. Choose a suitable model!')\n",
    "        \n",
    "    return y_true, y_pred, y_pred_lower, y_pred_upper, val_y_true, val_y_pred, val_y_pred_lower, val_y_pred_upper, og_data_true, og_data_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac60bcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for alpha in alphas:\n",
    "    \n",
    "    per_signal_ace_list, per_signal_ac_list, per_signal_pi_width_list, model_names_list_list = [], [], [], []\n",
    "    \n",
    "    for data_folder in data_folders:\n",
    "    \n",
    "        ace_list, ac_list, pi_width_list, model_names_list = [], [], [], []\n",
    "        \n",
    "        for model in models:\n",
    "\n",
    "            if model == 'transformer_enbpi':\n",
    "                ss = [5, 10, 20, 25, 50, 100]\n",
    "            else:\n",
    "                ss = [None]\n",
    "\n",
    "            for s in ss:\n",
    "\n",
    "                print(f\"Model: {model}, s: {s}, Alpha: {alpha}, Data Folder: {data_folder}\")\n",
    "\n",
    "                # Loading data\n",
    "                y_true, y_pred, y_pred_lower, y_pred_upper, val_y_true, val_y_pred, val_y_pred_lower, val_y_pred_upper, og_data_true, og_data_date = load_data(model=model, alpha=alpha, s=s, data_folder=data_folder)\n",
    "\n",
    "                ace = regression_ace(y_true, y_pred_lower, y_pred_upper, alpha)\n",
    "                ac = regression_mean_coverage(y_true, y_pred_lower, y_pred_upper)\n",
    "                pi_width = regression_sharpness(y_pred_lower, y_pred_upper)\n",
    "                \n",
    "                ace_list.append(ace)\n",
    "                ac_list.append(ac)\n",
    "                pi_width_list.append(pi_width)\n",
    "                model_names_list.append(model + '_' + str(s))\n",
    "                \n",
    "                \n",
    "                \n",
    "        per_signal_ace_list.append(np.concatenate((np.array([data_folder.split('_')[-1]]), ace_list)))\n",
    "        per_signal_ac_list.append(np.concatenate((np.array([data_folder.split('_')[-1]]), ac_list)))\n",
    "        per_signal_pi_width_list.append(np.concatenate((np.array([data_folder.split('_')[-1]]), pi_width_list)))\n",
    "        model_names_list_list.append(np.concatenate((np.array(['Metric']), model_names_list)))\n",
    "    \n",
    "    def save_lists_to_csv(list1, list_of_lists, filename):\n",
    "        with open(filename, 'w', newline='') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "\n",
    "            # Write data\n",
    "            for i, (item1, sublist) in enumerate(zip(list1, zip(*list_of_lists))):\n",
    "                row = [item1] + list(sublist)\n",
    "                csv_writer.writerow(row)\n",
    "\n",
    "    ace_filename = 'ace_' + str(alpha) + '.csv' \n",
    "    ac_filename = 'ac_' + str(alpha) + '.csv' \n",
    "    pi_width_filename = 'pi_width_' + str(alpha) + '.csv' \n",
    "    \n",
    "    save_lists_to_csv(model_names_list_list[-1], per_signal_ac_list, ac_filename)\n",
    "    save_lists_to_csv(model_names_list_list[-1], per_signal_ace_list, ace_filename)\n",
    "    save_lists_to_csv(model_names_list_list[-1], per_signal_pi_width_list, pi_width_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f66d12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for data_folder in data_folders:\n",
    "\n",
    "    for alpha in alphas:\n",
    "        \n",
    "        ace_list, ac_list, pi_width_list, model_names_list = [], [], [], []\n",
    "        \n",
    "        for model in models:\n",
    "\n",
    "            if model == 'transformer_enbpi':\n",
    "                ss = [5, 10, 20, 25, 50, 100]\n",
    "            else:\n",
    "                ss = [None]\n",
    "\n",
    "            for s in ss:\n",
    "\n",
    "                print(f\"Model: {model}, s: {s}, Alpha: {alpha}, Data Folder: {data_folder}\")\n",
    "\n",
    "                # Loading data\n",
    "                y_true, y_pred, y_pred_lower, y_pred_upper, val_y_true, val_y_pred, val_y_pred_lower, val_y_pred_upper, og_data_true, og_data_date = load_data(model=model, alpha=alpha, s=s, data_folder=data_folder)\n",
    "\n",
    "                ace = regression_ace(y_true, y_pred_lower, y_pred_upper, alpha)\n",
    "                ac = regression_mean_coverage(y_true, y_pred_lower, y_pred_upper)\n",
    "                pi_width = regression_sharpness(y_pred_lower, y_pred_upper)\n",
    "                \n",
    "                ace_list.append(ace)\n",
    "                ac_list.append(ac)\n",
    "                pi_width_list.append(pi_width)\n",
    "                model_names_list.append(model + '_' + str(s))\n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "        # Combine the lists\n",
    "        data = zip(model_names_list, ac_list, ace_list, pi_width_list)\n",
    "\n",
    "        # Specify the file name\n",
    "        filename = \".\\\\UpdatedPlots\\\\v1_exp1_metrics_\" +str(int((1-alpha)*100))+ \".csv\"\n",
    "\n",
    "        # Write the data to a CSV file\n",
    "        with open(filename, 'w', newline='') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            csvwriter.writerow(['Model + UQ', 'AC', 'ACE', 'PI-Width'])  # Writing header\n",
    "            csvwriter.writerows(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f354c5",
   "metadata": {},
   "source": [
    "**Metrics for the each UQ**\n",
    "\n",
    "- AC\n",
    "- ACE\n",
    "- PI-Width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd9e58c",
   "metadata": {},
   "source": [
    "*Plotting the metrics*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0821f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnCurrency(data_folder):\n",
    "    signal = data_folder.split('_')[-1]\n",
    "    \n",
    "    if signal == 'OT':\n",
    "        return 'GBP/USD'\n",
    "    elif signal == '1':\n",
    "        return 'SGD/USD'\n",
    "    elif signal == '2':\n",
    "        return 'JPY/USD'\n",
    "    elif signal == '3':\n",
    "        return 'CNY/USD'\n",
    "    elif signal == '5':\n",
    "        return 'CAD/USD'\n",
    "    elif signal == '6':\n",
    "        return 'AUD/USD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fcfc2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_folders = ['\\model_data_1','\\model_data_2','\\model_data_3','\\model_data_5','\\model_data_6','\\model_data_OT']\n",
    "model_names = ['ARIMA', 'ARIMA+GARCH', 'Transformer+MC-Dropout', 'Transformer+GARCH', 'Transformer+EnbPI (s=5)', 'Transformer+EnbPI (s=10)', 'Transformer+EnbPI (s=20)', 'Transformer+EnbPI (s=25)', 'Transformer+EnbPI (s=50)', 'Transformer+EnbPI (s=100)']\n",
    "\n",
    "for data_folder in data_folders:\n",
    "    \n",
    "    ace_list_list, ac_list_list, pi_width_list_list = [], [], []\n",
    "    \n",
    "    for model in models:\n",
    "        \n",
    "        \n",
    "\n",
    "        if model == 'transformer_enbpi':\n",
    "            ss = [5, 10, 20, 25, 50, 100]\n",
    "        else:\n",
    "            ss = [None]\n",
    "\n",
    "        for s in ss:\n",
    "            \n",
    "            ace_list, ac_list, pi_width_list = [], [], []\n",
    "\n",
    "            for alpha in alphas:\n",
    "                \n",
    "                print(f\"Model: {model}, s: {s}, Alpha: {alpha}, Data Folder: {data_folder}\")\n",
    "\n",
    "                # Loading data\n",
    "                y_true, y_pred, y_pred_lower, y_pred_upper, val_y_true, val_y_pred, val_y_pred_lower, val_y_pred_upper, og_data_true, og_data_date = load_data(model=model, alpha=alpha, s=s, data_folder=data_folder)\n",
    "\n",
    "                ace = regression_ace(y_true, y_pred_lower, y_pred_upper, alpha)\n",
    "                ac = regression_mean_coverage(y_true, y_pred_lower, y_pred_upper)\n",
    "                pi_width = regression_sharpness(y_pred_lower, y_pred_upper)\n",
    "                \n",
    "                ace_list.append(ace)\n",
    "                ac_list.append(ac)\n",
    "                pi_width_list.append(pi_width)\n",
    "                \n",
    "\n",
    "                \n",
    "            ace_list_list.append(np.array(ace_list))\n",
    "            ac_list_list.append(np.array(ac_list))\n",
    "            pi_width_list_list.append(np.array(pi_width_list))\n",
    "\n",
    "    plt.title('Average Coverage vs. Alpha When Forecasting ' +returnCurrency(data_folder))\n",
    "    plt.xlabel(r'$\\alpha$')\n",
    "    plt.ylabel('Coverage')\n",
    "    plt.grid()\n",
    "\n",
    "    for ind, ac in enumerate(ac_list_list):\n",
    "\n",
    "        conf = np.subtract(np.ones(4), alphas)\n",
    "        plt.plot(alphas, ac, label=model_names[ind], marker='o')\n",
    "\n",
    "    plt.plot(alphas, conf, label='Actual Confidence', linestyle='--', color='black', marker='^', alpha=0.3)\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.13), fancybox=True, shadow=False, ncol=3)\n",
    "    plt.xticks(alphas)\n",
    "    plt.savefig('.\\\\UpdatedPlots\\\\ACvsAlpha_'+data_folder.split('_')[-1]+'.png', bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    plt.title('PI-Width vs. Alpha When Forecasting ' +returnCurrency(data_folder))\n",
    "    plt.xlabel(r'$\\alpha$')\n",
    "    plt.ylabel('PI-Width')\n",
    "    plt.grid()\n",
    "    plt.xticks(alphas)\n",
    "\n",
    "    for ind, pi_width in enumerate(pi_width_list_list):\n",
    "\n",
    "        conf = np.subtract(np.ones(4), alphas)\n",
    "        plt.plot(alphas, pi_width, label=model_names[ind], marker='o')\n",
    "\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.13), fancybox=True, shadow=False, ncol=3)\n",
    "    plt.savefig('.\\\\UpdatedPlots\\\\PIvsAlpha_'+data_folder.split('_')[-1]+'.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85526579",
   "metadata": {},
   "source": [
    "*Plotting the volatility against the uncertatinty*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af383bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate sample time-series data\n",
    "np.random.seed(0)\n",
    "ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))\n",
    "\n",
    "# Define the window size for rolling volatility calculation\n",
    "window = 5\n",
    "\n",
    "# Calculate rolling volatility using the standard deviation\n",
    "rolling_volatility_std = ts.rolling(window).std()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "ts.plot(color='gray', label='Original Time-Series')\n",
    "rolling_volatility_std.plot(color='blue', label='Rolling Volatility (Standard Deviation)')\n",
    "\n",
    "plt.title('Rolling Volatility of a Time-Series')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1334a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_arrays_to_csv(model_name, s, alpha, uncertainty, std_volatility):\n",
    "    # Create the file name based on the model, s, and alpha\n",
    "    file_name = f\".\\\\UpdatedPlots\\\\uncertainty_vs_volatility\\\\uncertainty_vs_volatility_{model_name}_s{s}_alpha{alpha}.csv\"\n",
    "    \n",
    "    df = pd.DataFrame({\"uncertainty\" : uncertainty, \"std_volatility\" : std_volatility})\n",
    "    df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377dca98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "vol_window = 5\n",
    "model_names = ['ARIMA', 'ARIMA+GARCH', 'Transformer+MC-Dropout', 'Transformer+GARCH', 'Transformer+EnbPI (s=5)', 'Transformer+EnbPI (s=10)', 'Transformer+EnbPI (s=20)', 'Transformer+EnbPI (s=25)', 'Transformer+EnbPI (s=50)', 'Transformer+EnbPI (s=100)']\n",
    "ace_list_list, ac_list_list, pi_width_list_list = [], [], []\n",
    "\n",
    "\n",
    "\n",
    "for data_folder in data_folders:\n",
    "    i=0\n",
    "    \n",
    "    for ind, model in enumerate(models):\n",
    "        \n",
    "        \n",
    "\n",
    "        if model == 'transformer_enbpi':\n",
    "            ss = [5, 10, 20, 25, 50, 100]\n",
    "        else:\n",
    "            ss = [None]\n",
    "\n",
    "        for s in ss:\n",
    "            \n",
    "            ace_list, ac_list, pi_width_list = [], [], []\n",
    "\n",
    "           \n",
    "            \n",
    "            for alpha in alphas:\n",
    "                \n",
    "                print(f\"Model: {model}, s: {s}, Alpha: {alpha}, Data Folder: {data_folder}\")\n",
    "\n",
    "                # Loading data\n",
    "                y_true, y_pred, y_pred_lower, y_pred_upper, val_y_true, val_y_pred, val_y_pred_lower, val_y_pred_upper, og_data_true, og_data_date = load_data(model=model, alpha=alpha, s=s, data_folder=data_folder)\n",
    "\n",
    "                uncertainty = np.subtract(y_pred_upper, y_pred_lower)\n",
    "                \n",
    "                y_true_df = pd.DataFrame(y_true)\n",
    "                \n",
    "                std_volatility = y_true_df.rolling(vol_window).std()\n",
    "                \n",
    "                fig, ax1 = plt.subplots(figsize=(10,3))\n",
    "\n",
    "                ax1.plot(uncertainty, label='Model Uncertainty', color='blue',alpha=0.5, linewidth=0.8)\n",
    "                ax1.set_ylabel('Model Uncertainty')\n",
    "                ax1.set_xlim(0, len(uncertainty))\n",
    "                ax1.set_xlabel('Index')\n",
    "\n",
    "                ax2 = ax1.twinx()\n",
    "                ax2.plot(std_volatility, label='Volatility', color='red',alpha=0.5, linewidth=0.8)\n",
    "                ax2.set_ylabel('Volatility')\n",
    "                plt.xlabel('Index')\n",
    "                plt.grid(True)\n",
    "\n",
    "                fig.legend(loc='lower center', ncol=2, bbox_to_anchor=(0.5, -0.17))\n",
    "                #plt.title(f\"Model: {model}, s: {s}, Alpha: {alpha}\")\n",
    "                plt.title(f\"Model Uncertainty ({model_names[i]},α={alpha}) and Volatility for GBP/USD\")\n",
    "                filename = '.\\\\UpdatedPlots\\\\uncert_and_vol_' + model + '_s' +str(s)+ '_alpha_' + str(alpha) + '.png'  \n",
    "                plt.savefig(filename, bbox_inches='tight')                \n",
    "                plt.show()\n",
    "                \n",
    "                save_arrays_to_csv(model, s, alpha, uncertainty, std_volatility.values.flatten())\n",
    "            \n",
    "            i += 1\n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e621f9",
   "metadata": {},
   "source": [
    "*Table of portfolio matrix*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b741552",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = ['1', '2', '3', '5', '6', 'OT']\n",
    "model = 'arima_garch'\n",
    "s = None\n",
    "\n",
    "\n",
    "for signal in signals:\n",
    "\n",
    "    initial = True\n",
    "    table = []\n",
    "    \n",
    "    for strategy in ['1', '3']:\n",
    "\n",
    "        for alpha in alphas:\n",
    "            \n",
    "            # File paths\n",
    "            metrics_path = f\"C:\\\\Users\\\\porte\\\\Downloads\\\\Trading data with updated garch and commission\\\\trading_model_data_{signal}\\\\{model}_alpha{str(alpha)}_s{s}_optimisesharpe\\\\test_portfolio_metrics{strategy}.csv\"\n",
    "\n",
    "            # Read CSV files as DataFrames\n",
    "            metrics_df = pd.read_csv(metrics_path)\n",
    "            \n",
    "            if initial:\n",
    "                table.append(np.concatenate((np.array(['Metrics']), np.array(metrics_df['Metric']))))\n",
    "                initial = False\n",
    "           \n",
    "            table.append(np.concatenate((np.array([str(alpha)]), np.array(metrics_df['Mean']))))\n",
    "\n",
    "    # Transposing the array\n",
    "    data_transposed = np.array(table).T.tolist()\n",
    "\n",
    "    # Writing to CSV\n",
    "    filename = model + '_portfoliometrics_' + signal +'_v2.csv'\n",
    "    \n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(data_transposed)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd91f850",
   "metadata": {},
   "source": [
    "plot of sharpe ratio against alpha for all the signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249514c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHORT ONLY\n",
    "signals = ['1', '2', '3', '5', '6', 'OT']\n",
    "\n",
    "metrics = ['Cumulative Returns', 'Annualized Volatility', 'Sharpe Ratio', 'Sortino Ratio', 'Maximum Drawdown']\n",
    "\n",
    "model = 'arima_garch'\n",
    "s = None\n",
    "\n",
    "model = 'transformer_enbpi'\n",
    "s = 20\n",
    "\n",
    "strategy_name = ['Long', 'Short']\n",
    "signal_name = ['SGD/USD', 'JPY/USD', 'CNY/USD', 'CAD/USD', 'AUD/USD', 'GBP/USD', 'Average Line']\n",
    "\n",
    "\n",
    "\n",
    "for indx, metric in enumerate(metrics):\n",
    "    \n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=(5, 4)) \n",
    "    \n",
    "    for ind, strategy in enumerate(['1', '3']):\n",
    "\n",
    "        i=0\n",
    "        data = []\n",
    "\n",
    "        for signal in signals:\n",
    "\n",
    "            table = []\n",
    "\n",
    "            for alpha in alphas:\n",
    "\n",
    "                # File paths\n",
    "                metrics_path = f\"C:\\\\Users\\\\porte\\\\Downloads\\\\UpdatedPlots\\\\trading_model_data_{signal}\\\\{model}_alpha{str(alpha)}_s{s}_optimisesharpe\\\\test_portfolio_metrics{strategy}.csv\"\n",
    "\n",
    "                # Read CSV files as DataFrames\n",
    "                metrics_df = pd.read_csv(metrics_path)\n",
    "                \n",
    "                table.append(metrics_df['Mean'][indx])\n",
    "\n",
    "            data.append(np.array(table))\n",
    "\n",
    "            #if strategy == '1':\n",
    "            #    ax1.plot(alphas, table, label=signal_name[i], marker='o')\n",
    "            #    ax1.grid(True)\n",
    "            #    ax1.set_title('Sharpe Ratio Across α \\n for Long Strategy Using Model Certainty')\n",
    "            #    ax1.set_xlabel(r'$\\alpha$')\n",
    "            #    ax1.set_ylabel('Sharpe Ratio')\n",
    "            if strategy == '3':\n",
    "                ax1.plot(alphas, table, label=signal_name[i], marker='o')\n",
    "                ax1.grid(True)\n",
    "                ax1.set_title(metric + ' Across α \\n ARIMA + GARCH Model')\n",
    "                ax1.set_xlabel(r'$\\alpha$')\n",
    "                ax1.set_ylabel(metric)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        #if strategy == '1':\n",
    "        #    ax1.plot(alphas, np.mean(data, axis=0), label='Mean', marker='^', linestyle='--', color='black')\n",
    "        #    ax1.set_xticks(alphas)\n",
    "        if strategy == '3':\n",
    "            ax1.plot(alphas, np.mean(data, axis=0), label='Mean', marker='^', linestyle='--', color='black')\n",
    "            ax1.set_xticks(alphas)\n",
    "        \n",
    "    fig.legend(signal_name, loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.23))\n",
    "    plt.savefig('.\\\\UpdatedPlots\\\\ARIMA'+metric.replace(\" \", \"\")+'vsAlpha.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4acaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "alphas = [0.15, 0.1, 0.05, 0.01]\n",
    "\n",
    "# SHORT ONLY\n",
    "signals = ['1', '2', '3', '5', '6', 'OT']\n",
    "metrics = ['Cumulative Returns', 'Annualized Volatility', 'Sharpe Ratio', 'Sortino Ratio', 'Maximum Drawdown']\n",
    "\n",
    "model = 'arima_garch'\n",
    "s = None\n",
    "\n",
    "model = 'transformer_enbpi'\n",
    "s = 20\n",
    "\n",
    "strategy_name = ['Long', 'Short']\n",
    "signal_name = ['SGD/USD', 'JPY/USD', 'CNY/USD', 'CAD/USD', 'AUD/USD', 'GBP/USD', 'Average Line']\n",
    "\n",
    "# Create subplots: 3 rows, 2 columns\n",
    "fig, axs = plt.subplots(3, 2, figsize=(10, 12))#, gridspec_kw={'height_ratios': [1, 1, 1]})\n",
    "\n",
    "# Loop through metrics\n",
    "for metric_ind, metric in enumerate(metrics):\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for ind, strategy in enumerate(['1', '3']):\n",
    "        i = 0\n",
    "        for signal in signals:\n",
    "\n",
    "            table = []\n",
    "\n",
    "            for alpha in alphas:\n",
    "                # File paths\n",
    "                metrics_path = f\"C:\\\\Users\\\\porte\\\\Downloads\\\\UpdatedPlots\\\\trading_model_data_{signal}\\\\{model}_alpha{str(alpha)}_s{s}_optimisesharpe\\\\test_portfolio_metrics{strategy}.csv\"\n",
    "\n",
    "                # Read CSV files as DataFrames\n",
    "                metrics_df = pd.read_csv(metrics_path)\n",
    "\n",
    "                table.append(metrics_df['Mean'][metric_ind])\n",
    "\n",
    "            data.append(np.array(table))\n",
    "\n",
    "            row, col = divmod(metric_ind, 2)  # Determine subplot position\n",
    "\n",
    "            if strategy == '3':\n",
    "                axs[row, col].plot(alphas, table, label=signal_name[i], marker='o')\n",
    "                axs[row, col].grid(True)\n",
    "                axs[row, col].set_title(metric + ' Across α \\n Transformer + EnbPI Model')\n",
    "                axs[row, col].set_xlabel(r'$\\alpha$')\n",
    "                axs[row, col].set_ylabel(metric)\n",
    "            \n",
    "            i += 1\n",
    "\n",
    "    # Plot the mean line for strategy '3'\n",
    "    row, col = divmod(metric_ind, 2)\n",
    "    axs[row, col].plot(alphas, np.mean(data, axis=0), label='Mean', marker='^', linestyle='--', color='black')\n",
    "    axs[row, col].set_xticks(alphas)\n",
    "\n",
    "# Remove the last empty subplot and center the last plot\n",
    "fig.delaxes(axs[2, 1])\n",
    "\n",
    "# Adjust the width of the third row plot to span both columns\n",
    "axs[2, 0].set_position([0.2, 0, 0.5, 0.2])  # Manually adjust position: [left, bottom, width, height]\n",
    "\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "\n",
    "# Legend and layout\n",
    "fig.legend(signal_name, loc='lower center', ncol=3)\n",
    "plt.savefig('.\\\\UpdatedPlots\\\\TEmetricsvsAlpha.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ae55b4",
   "metadata": {},
   "source": [
    "buy and hold metrics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af6677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_portfolio_metrics(df, plot=True, havedate=True):\n",
    "        \n",
    "    if havedate:\n",
    "        # Converting the date column to datetime\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # Calculating the daily returns\n",
    "    df['daily_returns'] = df['portfolio_value'].pct_change()\n",
    "\n",
    "    # Calculating the cumulative returns\n",
    "    cumulative_returns = df['portfolio_value'].iloc[-1]/df['portfolio_value'].iloc[0] - 1\n",
    "    \n",
    "    annual_returns = (1+cumulative_returns)**(365/len(df['daily_returns'])) - 1 #https://www.investopedia.com/terms/a/annualized-total-return.asp\n",
    "\n",
    "    # Calculating the annualized volatility\n",
    "    annual_volatility = df['daily_returns'].std()*np.sqrt(len(df['daily_returns']))\n",
    "\n",
    "    # Risk-free rate is assumed to be 1%\n",
    "    risk_free_rate = 0.01/365\n",
    "    \n",
    "    sharpe_ratio = (annual_returns - risk_free_rate) / annual_volatility\n",
    "\n",
    "    # Calculating the sortino ratio\n",
    "    negative_returns = df[df['daily_returns']<0]['daily_returns']\n",
    "    downside_volatility = negative_returns.std()*np.sqrt(len(df['daily_returns']))\n",
    "    sortino_ratio = (annual_returns - risk_free_rate) / downside_volatility\n",
    "\n",
    "    # Calculating the beta alpha using linear regression\n",
    "    #benchmark_returns = SandPData.pct_change()[1:]\n",
    "    #(beta, alpha) = stats.linregress(benchmark_returns, df['daily_returns'][1:])[0:2]\n",
    "    #benchmark_returns = pd.Series(buy_and_hold(og_data['tradeactualPrice'])).pct_change()[1:]\n",
    "    #(beta, alpha) = stats.linregress(benchmark_returns, df['daily_returns'][1:])[0:2]\n",
    "\n",
    "    # Calculating the maximum drawdown\n",
    "    df['cumulative_returns'] = df['portfolio_value']/df['portfolio_value'].iloc[0]\n",
    "    df['peak'] = df['cumulative_returns'].cummax()\n",
    "    df['drawdown'] = df['cumulative_returns'] - df['peak']\n",
    "    max_drawdown = df['drawdown'].min()\n",
    "    \n",
    "    if plot:\n",
    "        # Plotting the portfolio value\n",
    "        sns.set(style='whitegrid')\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(df['Date'], df['portfolio_value'], linewidth=2)\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Portfolio Value')\n",
    "        plt.title('Portfolio Growth')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.xlim(df['Date'].iloc[0], df['Date'].iloc[-1]) \n",
    "\n",
    "        \n",
    "        y_min = df['portfolio_value'].min() * 0.95  \n",
    "        y_max = df['portfolio_value'].max() * 1.05  \n",
    "        plt.ylim(y_min, y_max)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        sns.despine()\n",
    "        plt.savefig(\"portfolio_growth.png\")\n",
    "        plt.show()\n",
    "\n",
    "        # Plotting maximum drawdown\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(df['Date'], df['drawdown'], color='red', linewidth=2)\n",
    "        plt.fill_between(df['Date'], df['drawdown'], color='lightcoral')  \n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Drawdown')\n",
    "        plt.title('Maximum Drawdown')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.xlim(df['Date'].iloc[0], df['Date'].iloc[-1])  \n",
    "\n",
    "        plt.tight_layout()\n",
    "        sns.despine()\n",
    "        plt.savefig(\"maximum_drawdown.png\")\n",
    "        plt.show()\n",
    "\n",
    "    # Removing temporary columns\n",
    "    df.drop(['daily_returns','cumulative_returns', 'peak', 'drawdown'], axis=1, inplace=True)\n",
    "\n",
    "    #return annual_returns, cumulative_returns, annual_volatility, sharpe_ratio, sortino_ratio, max_drawdown, beta, alpha\n",
    "    return cumulative_returns, annual_volatility, sharpe_ratio, sortino_ratio, max_drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f252bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buy_and_hold(priceData, initialCapital=1000, do_print=False, compute_metrics=False):\n",
    "\n",
    "    returns = priceData.pct_change()\n",
    "\n",
    "    returnsDf = pd.DataFrame(returns)\n",
    "    returnsDf.columns = ['returns']\n",
    "    \n",
    "    returnsDf = returnsDf.reset_index(drop=True)\n",
    "\n",
    "    fund_value = list()\n",
    "    fund_value.append(initialCapital-initialCapital*0.0001*0.2)\n",
    "\n",
    "    for i in range(len(priceData)):\n",
    "        if i>0:\n",
    "            if i == len(priceData)-1:\n",
    "                fund_value.append(fund_value[i-1]+fund_value[i-1]*returnsDf['returns'][i] + (fund_value[i-1]+fund_value[i-1]*returnsDf['returns'][i])*0.0001*0.2)\n",
    "            else:\n",
    "                fund_value.append(fund_value[i-1]+fund_value[i-1]*returnsDf['returns'][i])\n",
    "        \n",
    "    \n",
    "    if compute_metrics:\n",
    "        cumulative_returns, annual_volatility, sharpe_ratio, sortino_ratio, max_drawdown = calculate_portfolio_metrics(pd.DataFrame(fund_value, columns=['portfolio_value']), plot=False, havedate=False)\n",
    "    \n",
    "    if do_print:\n",
    "        print(\"Cumulative Returns:\", cumulative_returns)\n",
    "        print(\"Annualized Volatility:\", annual_volatility)\n",
    "        print(\"Sharpe Ratio:\", sharpe_ratio)\n",
    "        print(\"Sortino Ratio:\", sortino_ratio)\n",
    "        print(\"Maximum Drawdown:\", max_drawdown)\n",
    "        \n",
    "        return cumulative_returns, annual_volatility, sharpe_ratio, sortino_ratio, max_drawdown\n",
    "    else:   \n",
    "        return fund_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fbed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\porte\\Downloads\\dataset\\exchange_rate\\exchange_rate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce802c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = ['1', '2', '3', '5', '6', 'OT']\n",
    "\n",
    "table = []\n",
    "table.append(['Metrics', 'Cumulative Returns', 'Annualized Volatility', 'Sharpe Ratio', 'Sortino Ratio', 'Maximum Drawdown'])\n",
    "\n",
    "for ind, signal in enumerate(signals):\n",
    "    \n",
    "    cumulative_returns, annual_volatility, sharpe_ratio, sortino_ratio, max_drawdown = buy_and_hold(data[signal][-1518:], initialCapital=1000000, do_print=True, compute_metrics=True)\n",
    "    \n",
    "    table.append([signal_name[ind], cumulative_returns, annual_volatility, sharpe_ratio, sortino_ratio, max_drawdown])\n",
    "\n",
    "# Transposing the array\n",
    "data_transposed = np.array(table).T.tolist()\n",
    "\n",
    "# Writing to CSV\n",
    "filename = 'buyandhold.csv'\n",
    "\n",
    "with open(filename, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data_transposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc4d94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "signal = 'OT'\n",
    "model = 'transformer_enbpi'\n",
    "model_title = 'Transformer + EnbPI'\n",
    "s = 20\n",
    "alphas = [0.15, 0.1, 0.05, 0.01]\n",
    "\n",
    "#model = 'arima_garch'\n",
    "#model_title = 'ARIMA + GARCH'\n",
    "#s = None\n",
    "\n",
    "strategies = ['Long', 'Longusingthresholdedcertainty', 'Short', 'Shortusingthresholdedcertainty']\n",
    "strategieslegend = ['Long', 'Long using certainty', 'Short', 'Short using certainty']\n",
    "\n",
    "for alpha in alphas:\n",
    "    \n",
    "    #fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    #fig.subplots_adjust(wspace=0.3)  \n",
    "    \n",
    "    values = []\n",
    "    drawdown = []\n",
    "    \n",
    "    alpha = str(alpha)\n",
    "    \n",
    "    for strategy in strategies:\n",
    "\n",
    "        # File paths\n",
    "        portfolio_value_path = f\"C:\\\\Users\\\\porte\\\\Downloads\\\\UpdatedPlots\\\\trading_model_data_{signal}\\\\{model}_alpha{alpha}_s{s}_optimisesharpe\\\\portfolio_valueDf_{strategy}.csv\"\n",
    "\n",
    "        # Read CSV files as DataFrames\n",
    "        df = pd.read_csv(portfolio_value_path)\n",
    "        \n",
    "        # Calculating the maximum drawdown\n",
    "        df['cumulative_returns'] = df['portfolio_value']/df['portfolio_value'].iloc[0]\n",
    "        df['peak'] = df['cumulative_returns'].cummax()\n",
    "        df['drawdown'] = df['cumulative_returns'] - df['peak']\n",
    "        max_drawdown = df['drawdown'].min()\n",
    "        \n",
    "        values.append(np.array(df['portfolio_value']))\n",
    "        drawdown.append(np.array(df['drawdown']))\n",
    "        \n",
    "    for i in range(len(strategieslegend)):\n",
    "        \n",
    "        if i == 0:\n",
    "            \n",
    "            fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "            #ax1.plot(values[i], label=strategieslegend[i], color='blue', linewidth=0.9, linestyle='dotted')\n",
    "            #ax1.plot(values[i+1], label=strategieslegend[i+1], color='blue', linewidth=0.7)\n",
    "            ax1.plot(values[i+2], label=strategieslegend[i+2], color='black', linewidth=0.9, linestyle='dotted')\n",
    "            ax1.plot(values[i+3], label=strategieslegend[i+3], color='black', linewidth=0.7)\n",
    "            ax1.set_title('Equity Curve From ' + model_title + r' with $\\alpha$=' + alpha)\n",
    "            ax1.set_ylabel('Value ($)')\n",
    "            ax1.set_xlabel('Index')\n",
    "            ax1.set_xlim(0, len(values[i]))\n",
    "            ax1.grid(True)\n",
    "            fig.legend(['Without uncertainty', 'With certainty'], loc='lower center', ncol=2, bbox_to_anchor=(0.5, -0.23))\n",
    "            filename = '.\\\\UpdatedPlots\\\\transformer_equity_' + alpha + '_' + model + '.png' \n",
    "            plt.savefig(filename, bbox_inches='tight')\n",
    "            \n",
    "\n",
    "            # Create a second y-axis sharing the same x-axis\n",
    "            fig, ax3 = plt.subplots(figsize=(5, 3))\n",
    "            ax3.set_title('Drawdown From ' + model_title + r' with $\\alpha$=' + alpha)\n",
    "            ax3.fill_between(np.arange(0, len(drawdown[i+2])), drawdown[i+2], label='Without uncertainty', color='grey', alpha=0.3, linestyle='--', linewidth=1.5)\n",
    "            ax3.fill_between(np.arange(0, len(drawdown[i+3])), drawdown[i+3], label='With certainty', color='grey', alpha=0.6, linewidth=1.5)\n",
    "            ax3.set_ylabel('Drawdown')\n",
    "            ax3.set_xlabel('Index')\n",
    "            ax3.set_xlim(0, len(drawdown[i]))\n",
    "            ax3.set_ylim(min(drawdown[i+2])*1.1, 0)\n",
    "            ax3.grid(True)\n",
    "            fig.legend(['Without uncertainty', 'With certainty'], loc='lower center', ncol=2, bbox_to_anchor=(0.5, -0.23))\n",
    "            filename = '.\\\\UpdatedPlots\\\\transformer_drawdown_' + alpha + '_' + model + '.png' \n",
    "            plt.savefig(filename, bbox_inches='tight')\n",
    "            \n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a17cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "signal = 'OT'\n",
    "model = 'arima_garch'\n",
    "model_title = 'ARIMA + GARCH'\n",
    "s = None\n",
    "\n",
    "#model = 'arima_garch'\n",
    "#model_title = 'ARIMA + GARCH'\n",
    "#s = None\n",
    "\n",
    "strategies = ['Long', 'Longusingthresholdedcertainty', 'Short', 'Shortusingthresholdedcertainty']\n",
    "strategieslegend = ['Long', 'Long using certainty', 'Short', 'Short using certainty']\n",
    "\n",
    "for alpha in alphas:\n",
    "    \n",
    "    #fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    #fig.subplots_adjust(wspace=0.3)  \n",
    "    \n",
    "    values = []\n",
    "    drawdown = []\n",
    "    \n",
    "    alpha = str(alpha)\n",
    "    \n",
    "    for strategy in strategies:\n",
    "\n",
    "        # File paths\n",
    "        portfolio_value_path = f\"C:\\\\Users\\\\porte\\\\Downloads\\\\UpdatedPlots\\\\trading_model_data_{signal}\\\\{model}_alpha{alpha}_s{s}_optimisesharpe\\\\portfolio_valueDf_{strategy}.csv\"\n",
    "\n",
    "        # Read CSV files as DataFrames\n",
    "        df = pd.read_csv(portfolio_value_path)\n",
    "        \n",
    "        # Calculating the maximum drawdown\n",
    "        df['cumulative_returns'] = df['portfolio_value']/df['portfolio_value'].iloc[0]\n",
    "        df['peak'] = df['cumulative_returns'].cummax()\n",
    "        df['drawdown'] = df['cumulative_returns'] - df['peak']\n",
    "        max_drawdown = df['drawdown'].min()\n",
    "        \n",
    "        values.append(np.array(df['portfolio_value']))\n",
    "        drawdown.append(np.array(df['drawdown']))\n",
    "        \n",
    "    for i in range(len(strategieslegend)):\n",
    "        \n",
    "        if i == 0:\n",
    "            \n",
    "            fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "            #ax1.plot(values[i], label=strategieslegend[i], color='blue', linewidth=0.9, linestyle='dotted')\n",
    "            #ax1.plot(values[i+1], label=strategieslegend[i+1], color='blue', linewidth=0.7)\n",
    "            ax1.plot(values[i+2], label=strategieslegend[i+2], color='black', linewidth=0.9, linestyle='dotted')\n",
    "            ax1.plot(values[i+3], label=strategieslegend[i+3], color='black', linewidth=0.7)\n",
    "            ax1.set_title('Equity Curve From ' + model_title + r' with $\\alpha$=' + alpha)\n",
    "            ax1.set_ylabel('Value ($)')\n",
    "            ax1.set_xlabel('Index')\n",
    "            ax1.set_xlim(0, len(values[i]))\n",
    "            ax1.grid(True)\n",
    "            fig.legend(['Without uncertainty', 'With certainty'], loc='lower center', ncol=2, bbox_to_anchor=(0.5, -0.23))\n",
    "            filename = '.\\\\UpdatedPlots\\\\equity_' + alpha + '_' + model + '.png' \n",
    "            plt.savefig(filename, bbox_inches='tight')\n",
    "            \n",
    "            #ax1.fill_between(np.arange(0, len(drawdown[i])), drawdown[i], label=strategieslegend[i], color='lightcoral')\n",
    "            #ax3.set_title('Drawdown for ' + model_title + r' with $\\alpha$=' + alpha)\n",
    "            #ax3.legend()\n",
    "            #ax3.set_ylabel('Drawdown')\n",
    "            #ax3.set_xlabel('Index')\n",
    "            #ax3.grid(True)\n",
    "            \n",
    "            # Create a second y-axis sharing the same x-axis\n",
    "            fig, ax3 = plt.subplots(figsize=(5, 3))\n",
    "            ax3.set_title('Drawdown From ' + model_title + r' with $\\alpha$=' + alpha)\n",
    "            #ax3.fill_between(np.arange(0, len(drawdown[i])), drawdown[i], label='Long', color='blue', alpha=0.3, linestyle='--', linewidth=1.5)\n",
    "            #ax3.fill_between(np.arange(0, len(drawdown[i+1])), drawdown[i+1], label='Long with certainty', color='blue', alpha=0.6, linewidth=1.5)\n",
    "            ax3.fill_between(np.arange(0, len(drawdown[i+2])), drawdown[i+2], label='Without uncertainty', color='grey', alpha=0.3, linestyle='--', linewidth=1.5)\n",
    "            ax3.fill_between(np.arange(0, len(drawdown[i+3])), drawdown[i+3], label='With certainty', color='grey', alpha=0.6, linewidth=1.5)\n",
    "            ax3.set_ylabel('Drawdown')\n",
    "            ax3.set_xlabel('Index')\n",
    "            ax3.set_xlim(0, len(drawdown[i]))\n",
    "            ax3.set_ylim(min(drawdown[i+2])*1.1, 0)\n",
    "            ax3.grid(True)\n",
    "            fig.legend(['Without uncertainty', 'With certainty'], loc='lower center', ncol=2, bbox_to_anchor=(0.5, -0.23))\n",
    "            filename = '.\\\\UpdatedPlots\\\\drawdown_' + alpha + '_' + model + '.png' \n",
    "            plt.savefig(filename, bbox_inches='tight')\n",
    "            \n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8fb0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = ['1', '2', '3', '5', '6', 'OT']\n",
    "model = 'arima_garch'\n",
    "s = None\n",
    "\n",
    "\n",
    "for signal in signals:\n",
    "\n",
    "    initial = True\n",
    "    table = []\n",
    "    \n",
    "    for strategy in ['3']:\n",
    "\n",
    "        for alpha in alphas:\n",
    "            \n",
    "            # File paths\n",
    "            metrics_path = f\"C:\\\\Users\\\\porte\\\\Downloads\\\\UpdatedPlots\\\\trading_model_data_{signal}\\\\{model}_alpha{str(alpha)}_s{s}_optimisesharpe\\\\test_portfolio_metrics{strategy}.csv\"\n",
    "\n",
    "            # Read CSV files as DataFrames\n",
    "            metrics_df = pd.read_csv(metrics_path)\n",
    "            \n",
    "            if initial:\n",
    "                table.append(np.concatenate((np.array(['Metrics']), np.array(metrics_df['Metric']))))\n",
    "                initial = False\n",
    "           \n",
    "            table.append(np.concatenate((np.array([str(alpha)]), np.array(metrics_df['Mean']))))\n",
    "\n",
    "    # Transposing the array\n",
    "    data_transposed = np.array(table).T.tolist()\n",
    "\n",
    "    # Writing to CSV\n",
    "    filename = '.\\\\UpdatedPlots\\\\' + model + '_portfoliometrics_' + signal +'_v2.csv'\n",
    "    \n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(data_transposed)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a5d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = ['1', '2', '3', '5', '6', 'OT']\n",
    "model = 'transformer_enbpi'\n",
    "alphas = [0.15, 0.1, 0.05, 0.01]\n",
    "s = 20\n",
    "\n",
    "\n",
    "for signal in signals:\n",
    "\n",
    "    initial = True\n",
    "    table = []\n",
    "    \n",
    "    for strategy in ['3']:\n",
    "\n",
    "        for alpha in alphas:\n",
    "            \n",
    "            # File paths\n",
    "            metrics_path = f\"C:\\\\Users\\\\porte\\\\Downloads\\\\UpdatedPlots\\\\trading_model_data_{signal}\\\\{model}_alpha{str(alpha)}_s{s}_optimisesharpe\\\\test_portfolio_metrics{strategy}.csv\"\n",
    "\n",
    "            # Read CSV files as DataFrames\n",
    "            metrics_df = pd.read_csv(metrics_path)\n",
    "            \n",
    "            if initial:\n",
    "                table.append(np.concatenate((np.array(['Metrics']), np.array(metrics_df['Metric']))))\n",
    "                initial = False\n",
    "           \n",
    "            table.append(np.concatenate((np.array([str(alpha)]), np.array(metrics_df['Mean']))))\n",
    "\n",
    "    # Transposing the array\n",
    "    data_transposed = np.array(table).T.tolist()\n",
    "\n",
    "    # Writing to CSV\n",
    "    filename = '.\\\\UpdatedPlots\\\\' + model + '_portfoliometrics_' + signal +'_v2.csv'\n",
    "    \n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(data_transposed)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72ffbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Certainty\n",
    "signals = ['1', '2', '3', '5', '6', 'OT']\n",
    "model = 'arima_garch'\n",
    "s = None\n",
    "\n",
    "\n",
    "for signal in signals:\n",
    "\n",
    "    initial = True\n",
    "    table = []\n",
    "    \n",
    "    for strategy in ['2']:\n",
    "\n",
    "        \n",
    "            \n",
    "        # File paths\n",
    "        metrics_path = f\"C:\\\\Users\\\\porte\\\\Downloads\\\\UpdatedPlots\\\\trading_model_data_{signal}\\\\{model}_alpha{str(alpha)}_s{s}_optimisesharpe\\\\test_portfolio_metrics{strategy}.csv\"\n",
    "\n",
    "        # Read CSV files as DataFrames\n",
    "        metrics_df = pd.read_csv(metrics_path)\n",
    "\n",
    "        if initial:\n",
    "            table.append(np.concatenate((np.array(['Metrics']), np.array(metrics_df['Metric']))))\n",
    "            initial = False\n",
    "\n",
    "        table.append(np.concatenate((np.array([str(alpha)]), np.array(metrics_df['Mean']))))\n",
    "\n",
    "    # Transposing the array\n",
    "    data_transposed = np.array(table).T.tolist()\n",
    "\n",
    "    # Writing to CSV\n",
    "    filename = '.\\\\UpdatedPlots\\\\' + model + '_portfoliometrics_' + signal +'_no_certainty.csv'\n",
    "    \n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(data_transposed)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3319c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Certainty \n",
    "signals = ['1', '2', '3', '5', '6', 'OT']\n",
    "model = 'transformer_enbpi'\n",
    "s = 5\n",
    "\n",
    "\n",
    "for signal in signals:\n",
    "\n",
    "    initial = True\n",
    "    table = []\n",
    "    \n",
    "    for strategy in ['2']:\n",
    "\n",
    "        \n",
    "        # File paths\n",
    "        metrics_path = f\"C:\\\\Users\\\\porte\\\\Downloads\\\\UpdatedPlots\\\\trading_model_data_{signal}\\\\{model}_alpha{str(alpha)}_s{s}_optimisesharpe\\\\test_portfolio_metrics{strategy}.csv\"\n",
    "\n",
    "        # Read CSV files as DataFrames\n",
    "        metrics_df = pd.read_csv(metrics_path)\n",
    "\n",
    "        if initial:\n",
    "            table.append(np.concatenate((np.array(['Metrics']), np.array(metrics_df['Metric']))))\n",
    "            initial = False\n",
    "\n",
    "        table.append(np.concatenate((np.array([str(alpha)]), np.array(metrics_df['Mean']))))\n",
    "\n",
    "    # Transposing the array\n",
    "    data_transposed = np.array(table).T.tolist()\n",
    "\n",
    "    # Writing to CSV\n",
    "    filename = '.\\\\UpdatedPlots\\\\' + model + '_portfoliometrics_' + signal +'_no_certainty.csv'\n",
    "    \n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(data_transposed)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8044d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final tables:\n",
    "\n",
    "signals = ['1', '2', '3', '5', '6', 'OT']\n",
    "metrics = ['Cumulative Returns', 'Annualized Volatility', 'Sharpe Ratio', 'Sortino Ratio', 'Maximum Drawdown']\n",
    "alpha = 0.01\n",
    "initial = True\n",
    "table = []\n",
    "\n",
    "for signal in signals:\n",
    "\n",
    "    \n",
    "    \n",
    "    for strategy in ['3']:\n",
    "\n",
    "        for alpha_no in alphas:\n",
    "            \n",
    "            # File paths\n",
    "            transformer_enbpi_metrics_path = f\"C:\\\\Users\\\\porte\\\\Downloads\\\\UpdatedPlots\\\\trading_model_data_{signal}\\\\transformer_enbpi_alpha{alpha}_s20_optimisesharpe\\\\test_portfolio_metrics{strategy}.csv\"\n",
    "            arima_garch_metrics_path = f\"C:\\\\Users\\\\porte\\\\Downloads\\\\UpdatedPlots\\\\trading_model_data_{signal}\\\\arima_garch_alpha{alpha}_sNone_optimisesharpe\\\\test_portfolio_metrics{strategy}.csv\"\n",
    "\n",
    "            \n",
    "            # Read CSV files as DataFrames\n",
    "            transformer_enbpi_metrics_df = pd.read_csv(transformer_enbpi_metrics_path)\n",
    "            arima_garch_metrics_df = pd.read_csv(arima_garch_metrics_path)\n",
    "            \n",
    "            transformer_enbpi_metrics_df.set_index('Metric', inplace=True)\n",
    "            arima_garch_metrics_df.set_index('Metric', inplace=True)\n",
    "            \n",
    "    if initial:\n",
    "        # Create alternating headers for ARIMA and Transformer metrics\n",
    "        header = ['Signal']  # Start with 'Signal'\n",
    "        header += [item for metric in metrics for item in (f'arima_{metric}', f'transformer_{metric}')]  # Correct list comprehension\n",
    "\n",
    "        # Append the header to the table\n",
    "        table.append(np.array(header))\n",
    "\n",
    "        # Set initial to False after appending the header\n",
    "        initial = False\n",
    "        \n",
    "    \n",
    "    # Extract the metrics from both dataframes\n",
    "    arima_garch_metrics = arima_garch_metrics_df['Mean'][metrics].values\n",
    "    transformer_enbpi_metrics = transformer_enbpi_metrics_df['Mean'][metrics].values\n",
    "\n",
    "    # Alternate between ARIMA and Transformer metrics\n",
    "    alternating_metrics = np.array([val for pair in zip(arima_garch_metrics, transformer_enbpi_metrics) for val in pair])\n",
    "\n",
    "    # Concatenate the signal with the alternating metrics\n",
    "    row = np.concatenate(([signal], alternating_metrics))\n",
    "    \n",
    "    # Append the row to the table\n",
    "    table.append(row)\n",
    "\n",
    "filename = '.\\\\UpdatedPlots\\\\' + 'arima_garch_enbpi_portfoliometrics'+str(alpha)+'.csv'\n",
    "\n",
    "with open(filename, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(table)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15678301",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final tables (difference):\n",
    "\n",
    "signals = ['1', '2', '3', '5', '6', 'OT']\n",
    "metrics = ['Cumulative Returns', 'Annualized Volatility', 'Sharpe Ratio', 'Sortino Ratio', 'Maximum Drawdown']\n",
    "\n",
    "initial = True\n",
    "table = []\n",
    "\n",
    "for signal in signals:\n",
    "\n",
    "    \n",
    "    \n",
    "    for strategy in ['3']:\n",
    "\n",
    "        for alpha in alphas:\n",
    "            \n",
    "            # File paths\n",
    "            transformer_enbpi_metrics_path = f\"C:\\\\Users\\\\porte\\\\Downloads\\\\UpdatedPlots\\\\trading_model_data_{signal}\\\\transformer_enbpi_alpha0.05_s20_optimisesharpe\\\\test_portfolio_metrics{strategy}.csv\"\n",
    "            arima_garch_metrics_path = f\"C:\\\\Users\\\\porte\\\\Downloads\\\\UpdatedPlots\\\\trading_model_data_{signal}\\\\arima_garch_alpha0.05_sNone_optimisesharpe\\\\test_portfolio_metrics{strategy}.csv\"\n",
    "\n",
    "            \n",
    "            # Read CSV files as DataFrames\n",
    "            transformer_enbpi_metrics_df = pd.read_csv(transformer_enbpi_metrics_path)\n",
    "            arima_garch_metrics_df = pd.read_csv(arima_garch_metrics_path)\n",
    "            \n",
    "            transformer_enbpi_metrics_df.set_index('Metric', inplace=True)\n",
    "            arima_garch_metrics_df.set_index('Metric', inplace=True)\n",
    "            \n",
    "    for strategy in ['2']:\n",
    "\n",
    "        for alpha in alphas:\n",
    "            \n",
    "            # File paths\n",
    "            no_cert_transformer_enbpi_metrics_path = f\"C:\\\\Users\\\\porte\\\\Downloads\\\\UpdatedPlots\\\\trading_model_data_{signal}\\\\transformer_enbpi_alpha0.05_s20_optimisesharpe\\\\test_portfolio_metrics{strategy}.csv\"\n",
    "            no_cert_arima_garch_metrics_path = f\"C:\\\\Users\\\\porte\\\\Downloads\\\\UpdatedPlots\\\\trading_model_data_{signal}\\\\arima_garch_alpha0.05_sNone_optimisesharpe\\\\test_portfolio_metrics{strategy}.csv\"\n",
    "\n",
    "            \n",
    "            # Read CSV files as DataFrames\n",
    "            no_cert_transformer_enbpi_metrics_df = pd.read_csv(no_cert_transformer_enbpi_metrics_path)\n",
    "            no_cert_arima_garch_metrics_df = pd.read_csv(no_cert_arima_garch_metrics_path)\n",
    "            \n",
    "            no_cert_transformer_enbpi_metrics_df.set_index('Metric', inplace=True)\n",
    "            no_cert_arima_garch_metrics_df.set_index('Metric', inplace=True)\n",
    "            \n",
    "    if initial:\n",
    "        # Create alternating headers for ARIMA and Transformer metrics\n",
    "        header = ['Signal']  # Start with 'Signal'\n",
    "        header += [item for metric in metrics for item in (f'arima_{metric}', f'transformer_{metric}')]  # Correct list comprehension\n",
    "\n",
    "        # Append the header to the table\n",
    "        table.append(np.array(header))\n",
    "\n",
    "        # Set initial to False after appending the header\n",
    "        initial = False\n",
    "        \n",
    "    \n",
    "    # Extract the metrics from both dataframes\n",
    "    arima_garch_metrics = np.subtract(arima_garch_metrics_df['Mean'][metrics].values,no_cert_arima_garch_metrics_df['Mean'][metrics].values)\n",
    "    transformer_enbpi_metrics = np.subtract(transformer_enbpi_metrics_df['Mean'][metrics].values,no_cert_transformer_enbpi_metrics_df['Mean'][metrics].values)\n",
    "\n",
    "    # Alternate between ARIMA and Transformer metrics\n",
    "    alternating_metrics = np.array([val for pair in zip(arima_garch_metrics, transformer_enbpi_metrics) for val in pair])\n",
    "\n",
    "    # Concatenate the signal with the alternating metrics\n",
    "    row = np.concatenate(([signal], alternating_metrics))\n",
    "    \n",
    "    # Append the row to the table\n",
    "    table.append(row)\n",
    "\n",
    "filename = '.\\\\UpdatedPlots\\\\' + 'difference_arima_garch_enbpi_portfoliometrics.csv'\n",
    "\n",
    "with open(filename, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(table)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0dd67f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
