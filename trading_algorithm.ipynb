{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40670a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trading Algorithm \n",
    "# Author Samwel Portelli <samwel.portelli.18@um.edu.mt>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cde61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import sys\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Function that gets the daily close price data\n",
    "def get_data(symbol, start_date, end_date, interval = '1d', param='Close'):\n",
    "    \n",
    "    data = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n",
    "    closeData = data[param]\n",
    "    \n",
    "    return closeData\n",
    "\n",
    "SandPData = get_data('^GSPC', \"2006-08-16\",\"2010-10-10\", param='Adj Close')\n",
    "\n",
    "# Function to convert dates to the desired format\n",
    "def convert_to_datetime(date_str):\n",
    "    return datetime.strptime(date_str, '%Y/%m/%d %H:%M').strftime('%Y-%m-%d')\n",
    "\n",
    "def buy_and_hold(priceData, initialCapital=1000, do_print=False, compute_metrics=False):\n",
    "\n",
    "    returns = priceData.pct_change()\n",
    "\n",
    "    returnsDf = pd.DataFrame(returns)\n",
    "    returnsDf.columns = ['returns']\n",
    "\n",
    "    fund_value = list()\n",
    "    fund_value.append(initialCapital)\n",
    "\n",
    "    for i in range(len(priceData)):\n",
    "        if i>0:\n",
    "            fund_value.append(fund_value[i-1]+fund_value[i-1]*returnsDf['returns'][i])\n",
    "    \n",
    "    if compute_metrics:\n",
    "        cumulative_returns, annual_volatility, sharpe_ratio, sortino_ratio, max_drawdown, beta, alpha = calculate_portfolio_metrics(pd.DataFrame(fund_value, columns=['portfolio_value']), plot=False, havedate=False)\n",
    "    \n",
    "    if do_print:\n",
    "        print(\"Cumulative Returns:\", cumulative_returns)\n",
    "        print(\"Annualized Volatility:\", annual_volatility)\n",
    "        print(\"Sharpe Ratio:\", sharpe_ratio)\n",
    "        print(\"Sortino Ratio:\", sortino_ratio)\n",
    "        print(\"Beta:\", beta)\n",
    "        print(\"Alpha:\", alpha)\n",
    "        print(\"Maximum Drawdown:\", max_drawdown)\n",
    "    else:   \n",
    "        return fund_value\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456a86c2",
   "metadata": {},
   "source": [
    "*Code that generates a model that converts the certainty into threshold position size*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c582d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(data):\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    normalized_data = (data - min_val) / (max_val - min_val)\n",
    "    if np.all(np.isnan(normalized_data)):\n",
    "        normalized_data = np.zeros_like(data)\n",
    "    return normalized_data\n",
    "\n",
    "def get_lin_reg_dataset(df, step=30, test=False, val_df=None):\n",
    "    \n",
    "    x, y = [], []\n",
    "    \n",
    "    if test:\n",
    "        for i in range(step):\n",
    "            con_x = np.concatenate((val_df['certainty'][len(val_df['certainty']) - step + i:len(val_df['certainty'])].values, df['certainty'][0:i].values))\n",
    "                \n",
    "            x.append(min_max_normalize(con_x))\n",
    "            y.append(1-np.abs(df['actualClass'][i]-df['predClass'][i])) # 1 means trade, 0 means no trade\n",
    "        \n",
    "    \n",
    "    for i in range(len(df['certainty'])-step):\n",
    "        x.append(min_max_normalize(df['certainty'][i:i+step].values))\n",
    "        y.append(1-np.abs(df['actualClass'][i+step]-df['predClass'][i+step])) # 1 means trade, 0 means no trade\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(X_train, y_train):\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# Function to test the model and classify based on a threshold\n",
    "def test_model_with_threshold(model, X_test, y_test, threshold=0.5):\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]  # Probability of positive class    \n",
    "    y_pred = (y_proba >= threshold).astype(int)  # Classify based on threshold\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return y_proba, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa28009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_portfolio_metrics(df, og_data, plot=True, havedate=True):\n",
    "        \n",
    "    if havedate:\n",
    "        # Converting the date column to datetime\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # Calculating the daily returns\n",
    "    df['daily_returns'] = df['portfolio_value'].pct_change()\n",
    "\n",
    "    # Calculating the cumulative returns\n",
    "    cumulative_returns = df['portfolio_value'].iloc[-1]/df['portfolio_value'].iloc[0] - 1\n",
    "    \n",
    "    annual_returns = (1+cumulative_returns)**(365/len(df['daily_returns'])) - 1 #https://www.investopedia.com/terms/a/annualized-total-return.asp\n",
    "\n",
    "    # Calculating the annualized volatility\n",
    "    annual_volatility = df['daily_returns'].std()*np.sqrt(len(df['daily_returns']))\n",
    "\n",
    "    # Risk-free rate is assumed to be 1%\n",
    "    risk_free_rate = 0.01/365\n",
    "\n",
    "    # Calculating the sharpe ratio\n",
    "    print('**TEST**')\n",
    "    print('annual_returns: '+str(annual_returns))\n",
    "    print('risk_free_rate: '+str(risk_free_rate))\n",
    "    print('annual_volatility: '+str(annual_volatility))\n",
    "    print('**TEST**')\n",
    "    \n",
    "    sharpe_ratio = (annual_returns - risk_free_rate) / annual_volatility\n",
    "\n",
    "    # Calculating the sortino ratio\n",
    "    negative_returns = df[df['daily_returns']<0]['daily_returns']\n",
    "    downside_volatility = negative_returns.std()*np.sqrt(len(df['daily_returns']))\n",
    "    sortino_ratio = (annual_returns - risk_free_rate) / downside_volatility\n",
    "\n",
    "    # Calculating the beta alpha using linear regression\n",
    "    #benchmark_returns = SandPData.pct_change()[1:]\n",
    "    #(beta, alpha) = stats.linregress(benchmark_returns, df['daily_returns'][1:])[0:2]\n",
    "    benchmark_returns = pd.Series(buy_and_hold(og_data['tradeactualPrice'])).pct_change()[1:]\n",
    "    (beta, alpha) = stats.linregress(benchmark_returns, df['daily_returns'][1:])[0:2]\n",
    "\n",
    "    # Calculating the maximum drawdown\n",
    "    df['cumulative_returns'] = df['portfolio_value']/df['portfolio_value'].iloc[0]\n",
    "    \n",
    "    #df['peak'] = df['cumulative_returns'].cummax()\n",
    "    df['peak'] = df['portfolio_value'].cummax()\n",
    "    df['drawdown'] = (df['portfolio_value'] - df['peak'])/df['peak']\n",
    "    max_drawdown = df['drawdown'].min()*100\n",
    "    \n",
    "    if plot:\n",
    "        # Plotting the portfolio value\n",
    "        sns.set(style='whitegrid')\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(df['Date'], df['portfolio_value'], linewidth=2)\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Portfolio Value')\n",
    "        plt.title('Portfolio Growth')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.xlim(df['Date'].iloc[0], df['Date'].iloc[-1]) \n",
    "\n",
    "        \n",
    "        y_min = df['portfolio_value'].min() * 0.95  \n",
    "        y_max = df['portfolio_value'].max() * 1.05  \n",
    "        plt.ylim(y_min, y_max)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        sns.despine()\n",
    "        plt.savefig(\"portfolio_growth.png\")\n",
    "        plt.show()\n",
    "\n",
    "        # Plotting maximum drawdown\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(df['Date'], df['drawdown'], color='red', linewidth=2)\n",
    "        plt.fill_between(df['Date'], df['drawdown'], color='lightcoral')  \n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Drawdown')\n",
    "        plt.title('Maximum Drawdown')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.xlim(df['Date'].iloc[0], df['Date'].iloc[-1])  \n",
    "\n",
    "        plt.tight_layout()\n",
    "        sns.despine()\n",
    "        plt.savefig(\"maximum_drawdown.png\")\n",
    "        plt.show()\n",
    "\n",
    "    # Removing temporary columns\n",
    "    df.drop(['daily_returns','cumulative_returns', 'peak', 'drawdown'], axis=1, inplace=True)\n",
    "\n",
    "    #return annual_returns, cumulative_returns, annual_volatility, sharpe_ratio, sortino_ratio, max_drawdown, beta, alpha\n",
    "    return cumulative_returns, annual_volatility, sharpe_ratio, sortino_ratio, max_drawdown, beta, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unique_folder(folder_name):\n",
    "    version = 1\n",
    "    new_folder_name = folder_name\n",
    "    while os.path.exists(new_folder_name):\n",
    "        new_folder_name = f\"{folder_name}_v{version}\"\n",
    "        version += 1\n",
    "    os.makedirs(new_folder_name)\n",
    "    return new_folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aee45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_as_df(file_name, folder_name):\n",
    "    file_path = os.path.join(folder_name, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"File '{file_name}' does not exist in folder '{folder_name}'.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad82a5b6",
   "metadata": {},
   "source": [
    "*Loading the data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd6e0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(model, alpha, s=None, data_folder='\\model_data_OT'):\n",
    "    \n",
    "    folder_path = r'C:\\Users\\porte\\Downloads' + data_folder\n",
    "    \n",
    "    if model=='transformer_enbpi':\n",
    "\n",
    "        file_path = os.path.join(folder_path, f'transformer_confidence_run_data_s_{s}', f'ALL_PI_data_s_{s}.csv')\n",
    "        file_path_true_pred = os.path.join(folder_path, f'transformer_confidence_run_data_s_{s}', f'y_true_and_pred_s_{s}.csv')\n",
    "        og_data = r'C:\\Users\\porte\\Downloads\\dataset\\exchange_rate\\exchange_rate.csv'\n",
    "        val_file_path = os.path.join(folder_path, f'transformer_confidence_run_data_s_{s}_val', f'ALL_PI_data_s_{s}.csv')\n",
    "        val_file_path_true_pred = os.path.join(folder_path, f'transformer_confidence_run_data_s_{s}_val', f'y_true_and_pred_s_{s}.csv')\n",
    "    \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path, delimiter=',')\n",
    "\n",
    "        # Iterate through each column and convert it to an array\n",
    "        confidence_data = {}\n",
    "        for column in df.columns:\n",
    "            confidence_data[column] = df[column].to_numpy()\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path_true_pred, delimiter=',')\n",
    "\n",
    "        # Iterate through each column and convert it to an array\n",
    "        true_pred = {}\n",
    "        for column in df.columns:\n",
    "            true_pred[column] = df[column].to_numpy()\n",
    "\n",
    "        # VALIDATION DATA\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(val_file_path, delimiter=',')\n",
    "\n",
    "        # Iterate through each column and convert it to an array\n",
    "        val_confidence_data = {}\n",
    "        for column in df.columns:\n",
    "            val_confidence_data[column] = df[column].to_numpy()\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(val_file_path_true_pred, delimiter=',')\n",
    "\n",
    "        # Iterate through each column and convert it to an array\n",
    "        val_true_pred = {}\n",
    "        for column in df.columns:\n",
    "            val_true_pred[column] = df[column].to_numpy()\n",
    "\n",
    "        # ORIGINAL DATA\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(og_data, delimiter=',')\n",
    "\n",
    "        # Iterate through each column and convert it to an array\n",
    "        og_data = {}\n",
    "        for column in df.columns:\n",
    "            og_data[column] = df[column].to_numpy()\n",
    "\n",
    "        #Test Set\n",
    "        y_true=true_pred['y_true']\n",
    "        y_pred=true_pred['y_pred_from_PUNCC']\n",
    "        y_pred_lower=confidence_data['low_'+str(alpha)]\n",
    "        y_pred_upper=confidence_data['high_'+str(alpha)]\n",
    "\n",
    "        # Validation Set\n",
    "        val_y_true=val_true_pred['y_true']\n",
    "        val_y_pred=val_true_pred['y_pred_from_PUNCC']\n",
    "        val_y_pred_lower=val_confidence_data['low_'+str(alpha)]\n",
    "        val_y_pred_upper=val_confidence_data['high_'+str(alpha)]\n",
    "\n",
    "        og_data_true=og_data[data_folder.split('_')[-1]]\n",
    "        og_data_date=og_data['date']\n",
    "    \n",
    "    elif model=='transformer_garch':\n",
    "        conf = int(100-alpha*100)\n",
    "        \n",
    "        file_path = os.path.join(folder_path, f'garch_transformer_confidence_run_data_alpha_{alpha}', f'garch_transforemer_y_true_and_pred_alp_{conf}_{data_folder.split(\"_\")[-1]}.csv')\n",
    "        #file_path = r'C:\\Users\\porte\\Downloads'+str(data_folder)+'\\\\garch_transformer_confidence_run_data_alpha_'+str(alpha)+'\\garch_transforemer_y_true_and_pred_alp_'+str(conf)+'.csv'\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        y_true = df['y_true'].values\n",
    "        y_pred = df['y_pred_from_PUNCC'].values\n",
    "        y_pred_lower = df['y_lower'].values\n",
    "        y_pred_upper = df['y_upper'].values\n",
    "\n",
    "        file_path = os.path.join(folder_path, f'val_garch_transformer_confidence_run_data_alpha_{alpha}', f'val_garch_transforemer_y_true_and_pred_alp_{conf}_{data_folder.split(\"_\")[-1]}.csv')\n",
    "        #file_path = r'C:\\Users\\porte\\Downloads'+str(data_folder)+'\\\\val_garch_transformer_confidence_run_data_alpha_'+str(alpha)+'\\garch_transforemer_y_true_and_pred_alp_'+str(conf)+'.csv'\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        val_y_true = df['y_true'].values\n",
    "        val_y_pred = df['y_pred_from_PUNCC'].values\n",
    "        val_y_pred_lower = df['y_lower'].values\n",
    "        val_y_pred_upper = df['y_upper'].values\n",
    "\n",
    "        og_data = r'C:\\Users\\porte\\Downloads\\dataset\\exchange_rate\\exchange_rate.csv'\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(og_data, delimiter=',')\n",
    "\n",
    "        # Iterate through each column and convert it to an array\n",
    "        og_data = {}\n",
    "        for column in df.columns:\n",
    "            og_data[column] = df[column].to_numpy()\n",
    "\n",
    "        og_data_true=og_data[data_folder.split('_')[-1]]\n",
    "        og_data_date=og_data['date'] \n",
    "        \n",
    "    elif model=='transformer_mcdropout':\n",
    "        conf = int(100-alpha*100)\n",
    "\n",
    "        file_path = os.path.join(folder_path, 'output_mc_1_MCDROPOUT_PREDICTION', 'output_mc_1',f'min_max_mean_alpha_{alpha}.csv')\n",
    "        #file_path = r'C:\\Users\\porte\\Downloads'+str(data_folder)+'\\\\output_mc_1_MCDROPOUT_PREDICTION\\output_mc_1\\min_max_mean_alpha_'+str(alpha)+'.csv'\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        file_path = os.path.join(folder_path, 'output_mc_1_MCDROPOUT_PREDICTION', 'output_mc_1','true_0.npy')\n",
    "        \n",
    "        #y_true = np.load(r'C:\\Users\\porte\\Downloads'+str(data_folder)+'\\\\output_mc_1_MCDROPOUT_PREDICTION\\output_mc_1\\true_0.npy').flatten()\n",
    "        y_true = np.load(file_path).flatten()\n",
    "        y_pred = df['mean'].values\n",
    "        y_pred_lower = df['lower'].values\n",
    "        y_pred_upper = df['upper'].values\n",
    "\n",
    "\n",
    "        file_path = os.path.join(folder_path, 'output_mc_1_MCDROPOUT_VAL', 'output_mc_1',f'min_max_mean_alpha_{alpha}.csv')\n",
    "        #file_path = r'C:\\Users\\porte\\Downloads'+str(data_folder)+'\\\\output_mc_1_MCDROPOUT_VAL\\output_mc_1\\min_max_mean_alpha_'+str(alpha)+'.csv'\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        file_path = os.path.join(folder_path, 'output_mc_1_MCDROPOUT_VAL', 'output_mc_1','true_0.npy')\n",
    "        \n",
    "        #val_y_true = np.load(r'C:\\Users\\porte\\Downloads'+str(data_folder)+'\\\\output_mc_1_MCDROPOUT_VAL\\output_mc_1\\true_0.npy').flatten()\n",
    "        val_y_true = np.load(file_path).flatten()\n",
    "        \n",
    "        #val_y_true = np.load(r'C:\\Users\\porte\\Downloads'+str(data_folder)+'\\\\output_mc_1_MCDROPOUT_VAL\\output_mc_1\\true_0.npy').flatten()\n",
    "        val_y_pred = df['mean'].values\n",
    "        val_y_pred_lower = df['lower'].values\n",
    "        val_y_pred_upper = df['upper'].values\n",
    "\n",
    "        og_data = r'C:\\Users\\porte\\Downloads\\dataset\\exchange_rate\\exchange_rate.csv'\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(og_data, delimiter=',')\n",
    "\n",
    "        # Iterate through each column and convert it to an array\n",
    "        og_data = {}\n",
    "        for column in df.columns:\n",
    "            og_data[column] = df[column].to_numpy()\n",
    "\n",
    "        og_data_true=og_data[data_folder.split('_')[-1]]\n",
    "        og_data_date=og_data['date']        \n",
    "    elif model=='arima_garch':\n",
    "        conf = int(100-alpha*100)\n",
    "\n",
    "        data_file_path = r'C:\\Users\\porte\\Downloads'+str(data_folder)+'\\\\arimagarch_y_true_and_pred_alp_'+str(conf)+'.csv'\n",
    "\n",
    "        # Read data CSV file\n",
    "        df_data = pd.read_csv(data_file_path, delimiter=',')\n",
    "\n",
    "        # Store columns in arrays\n",
    "        y_true = df_data['y_true'].values\n",
    "        y_pred = df_data['y_pred_from_arimagarch'].values \n",
    "        y_pred_lower = df_data['y_lower'].values\n",
    "        y_pred_upper = df_data['y_upper'].values\n",
    "\n",
    "        data_file_path = r'C:\\Users\\porte\\Downloads'+str(data_folder)+'\\\\val_arimagarch_y_true_and_pred_alp_'+str(conf)+'.csv'\n",
    "\n",
    "        # Read data CSV file\n",
    "        df_data = pd.read_csv(data_file_path, delimiter=',')\n",
    "\n",
    "        # Store columns in arrays\n",
    "        val_y_true = df_data['y_true'].values\n",
    "        val_y_pred = df_data['y_pred_from_arimagarch'].values \n",
    "        val_y_pred_lower = df_data['y_lower'].values\n",
    "        val_y_pred_upper = df_data['y_upper'].values\n",
    "\n",
    "        og_data = r'C:\\Users\\porte\\Downloads\\dataset\\exchange_rate\\exchange_rate.csv'\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(og_data, delimiter=',')\n",
    "\n",
    "        # Iterate through each column and convert it to an array\n",
    "        og_data = {}\n",
    "        for column in df.columns:\n",
    "            og_data[column] = df[column].to_numpy()\n",
    "\n",
    "        og_data_true=og_data[data_folder.split('_')[-1]]\n",
    "        og_data_date=og_data['date']        \n",
    "    elif model=='arima':\n",
    "        conf = int(100-alpha*100)\n",
    "\n",
    "        data_file_path = r'C:\\Users\\porte\\Downloads'+str(data_folder)+'\\\\arima_y_true_and_pred_alp_'+str(conf)+'.csv'\n",
    "\n",
    "        # Read data CSV file\n",
    "        df_data = pd.read_csv(data_file_path, delimiter=',')\n",
    "\n",
    "        # Store columns in arrays\n",
    "        y_true = df_data['y_true'].values\n",
    "        y_pred = df_data['y_pred_from_arima'].values \n",
    "        y_pred_lower = df_data['y_lower'].values\n",
    "        y_pred_upper = df_data['y_upper'].values\n",
    "\n",
    "        data_file_path = r'C:\\Users\\porte\\Downloads'+str(data_folder)+'\\\\val_arima_y_true_and_pred_alp_'+str(conf)+'.csv'\n",
    "\n",
    "        # Read data CSV file\n",
    "        df_data = pd.read_csv(data_file_path, delimiter=',')\n",
    "\n",
    "        # Store columns in arrays\n",
    "        val_y_true = df_data['y_true'].values\n",
    "        val_y_pred = df_data['y_pred_from_arima'].values \n",
    "        val_y_pred_lower = df_data['y_lower'].values\n",
    "        val_y_pred_upper = df_data['y_upper'].values\n",
    "\n",
    "        og_data = r'C:\\Users\\porte\\Downloads\\dataset\\exchange_rate\\exchange_rate.csv'\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(og_data, delimiter=',')\n",
    "\n",
    "        # Iterate through each column and convert it to an array\n",
    "        og_data = {}\n",
    "        for column in df.columns:\n",
    "            og_data[column] = df[column].to_numpy()\n",
    "\n",
    "        og_data_true=og_data[data_folder.split('_')[-1]]\n",
    "        og_data_date=og_data['date']        \n",
    "    else:\n",
    "        print('Model does not exist. Choose a suitable model!')\n",
    "        \n",
    "    return y_true, y_pred, y_pred_lower, y_pred_upper, val_y_true, val_y_pred, val_y_pred_lower, val_y_pred_upper, og_data_true, og_data_date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bd593f",
   "metadata": {},
   "source": [
    "*Preparing the data for the trading algorithm*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae7717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(y_true, val_y_true, og_data_date, og_data_true):\n",
    "\n",
    "    #Test Set\n",
    "    index = np.arange(0,len(y_true))\n",
    "    tradedate = og_data_date[-len(y_true)-1:-1]\n",
    "    date = og_data_date[-len(y_true):]\n",
    "    tradeactualPrice = og_data_true[-len(y_true)-1:-1]\n",
    "    actualPrice = y_true\n",
    "\n",
    "    # Validation Set\n",
    "    val_index = np.arange(0,len(val_y_true))\n",
    "    val_tradedate = og_data_date[-len(val_y_true)-1-len(y_true):-1-len(y_true)]\n",
    "    val_date = og_data_date[-len(val_y_true)-len(y_true):-len(y_true)]\n",
    "    val_tradeactualPrice = og_data_true[-len(val_y_true)-1-len(y_true):-1-len(y_true)]\n",
    "    val_actualPrice = og_data_true[-len(val_y_true)-len(y_true):-len(y_true)]#val_y_true\n",
    "    \n",
    "    return index, tradedate, date, tradeactualPrice, actualPrice, val_index, val_tradedate, val_date, val_tradeactualPrice, val_actualPrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2660802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_class(index, actualPrice, tradeactualPrice, y_pred):\n",
    "    \n",
    "    actualClass, predClass = [], []\n",
    "\n",
    "    for i in index:\n",
    "\n",
    "        # Getting the ACTUAL signal\n",
    "        if actualPrice[i] > tradeactualPrice[i]:\n",
    "            actualClass.append(1)\n",
    "        else:\n",
    "            actualClass.append(0)\n",
    "\n",
    "        # Getting the PREDICTED signal\n",
    "        if y_pred[i] > tradeactualPrice[i]:\n",
    "            predClass.append(1)\n",
    "        else:\n",
    "            predClass.append(0)\n",
    "    \n",
    "    return np.array(actualClass), np.array(predClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e94f379",
   "metadata": {},
   "source": [
    "*Normalising the uncertainty of the test set using params obtained from the val set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae5196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the uncertainty of the validation dataset so that I can obtain the normalising parameters for\n",
    "# the test set uncertainty\n",
    "\n",
    "def normalise_test_uncert(val_y_pred_upper, val_y_pred_lower, y_pred_upper, y_pred_lower):\n",
    "\n",
    "    val_uncert = np.subtract(val_y_pred_upper, val_y_pred_lower)\n",
    "\n",
    "    val_uncert_min = np.min(val_uncert)\n",
    "    val_uncert_max = np.max(val_uncert)\n",
    "\n",
    "    test_uncert = np.subtract(y_pred_upper, y_pred_lower)\n",
    "\n",
    "    norm_test_uncert = (test_uncert - val_uncert_min) / (val_uncert_max - val_uncert_min)\n",
    "    norm_val_uncert = (val_uncert - val_uncert_min) / (val_uncert_max - val_uncert_min)\n",
    "    \n",
    "    # !!SKIPPING NORMALISATION!!\n",
    "    norm_val_uncert = val_uncert\n",
    "    norm_test_uncert = test_uncert\n",
    "    \n",
    "    return norm_val_uncert, norm_test_uncert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258de528",
   "metadata": {},
   "source": [
    "*Converting the dates to datetime format*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a613191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dates(tradedate, date, val_tradedate, val_date):\n",
    "\n",
    "    # Convert each date in the array to the desired format\n",
    "    tradedate = [convert_to_datetime(dt) for dt in tradedate]\n",
    "    date = [convert_to_datetime(dt) for dt in date]\n",
    "\n",
    "    # Validation\n",
    "    val_tradedate = [convert_to_datetime(dt) for dt in val_tradedate]\n",
    "    val_date = [convert_to_datetime(dt) for dt in val_date]\n",
    "    \n",
    "    return tradedate, date, val_tradedate, val_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f842fd",
   "metadata": {},
   "source": [
    "*Skip day ?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182a0cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_day(df, threshold, days=5):\n",
    "    \n",
    "    # Initialising the signal\n",
    "    df['skip'] = 0\n",
    "    \n",
    "    # Iterate over the DataFrame and set 'skip' column values\n",
    "    for i in range(len(df) - days):\n",
    "        if df.loc[i, 'certainty'] > threshold:\n",
    "            df.loc[i:i+days, 'skip'] = 1\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cf2290",
   "metadata": {},
   "source": [
    "*Setting up the dataframe for the trading algorithm*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ae6a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(index, tradedate, date, tradeactualPrice, actualPrice, actualClass, predClass, certainty):\n",
    "\n",
    "    data ={'index':index,'tradedate':tradedate,'date':date,'tradeactualPrice':tradeactualPrice,'actualPrice':actualPrice,'actualClass':actualClass,'predClass':predClass,'certainty':certainty}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa20e8c",
   "metadata": {},
   "source": [
    "*Trading Algorithm Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dd03db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## updated with a trading commission instead of fee, like IBRKs\n",
    "def tradingStrategy(df,portfolio_value = 1000, useCertainty = True, usetransactionFee = True, transactionFee = 0.005, useSlippage = True, slippageChange = 0.1, certaintyThreshold = 0.1, short=False, skip=False):\n",
    "    \n",
    "    # Extracting the unique trading dates\n",
    "    tradingDates = df.tradedate.unique()\n",
    "    \n",
    "    portfolio_value_list = list()\n",
    "    remainingLiquid = 0 \n",
    "    \n",
    "    # Trading on each trading day\n",
    "    for tradingDate in tradingDates:\n",
    "        \n",
    "        # Storing the data that will later be used for evaluation\n",
    "        portfolio_value_list.append([tradingDate, portfolio_value])\n",
    "        \n",
    "        if skip:\n",
    "            # If skip signal is 0 this means that trade normal\n",
    "            if df.query('tradedate==\"%s\"' % (tradingDate))['skip'].values[0] == 0:\n",
    "                # Filtering the datafram based on whether short positons wil be considered or not (if yes Sell signals are not neglected)\n",
    "                if short:\n",
    "                    dayDf = df.query('tradedate==\"%s\" & (predClass==1 | predClass==0) & certainty < %f' % (tradingDate, float(certaintyThreshold)))\n",
    "                else:\n",
    "                    dayDf = df.query('tradedate==\"%s\" & predClass==1 & certainty < %f' % (tradingDate, float(certaintyThreshold)))\n",
    "            else: #If skip = 1, then no trades shall occur. Hence create a dataframe with no trades\n",
    "                dayDf = pd.DataFrame(columns=['index', 'tradedate', 'date', 'tradeactualPrice', 'actualPrice', 'actualClass', 'predClass', 'certainty', 'skip'])\n",
    "        else:\n",
    "            if short:\n",
    "                dayDf = df.query('tradedate==\"%s\" & (predClass==1 | predClass==0) & certainty < %f' % (tradingDate, float(certaintyThreshold)))\n",
    "            else:\n",
    "                dayDf = df.query('tradedate==\"%s\" & predClass==1 & certainty < %f' % (tradingDate, float(certaintyThreshold)))\n",
    "        \n",
    "        # Check whether there a profitable at the current trading day\n",
    "        if dayDf.shape[0] > 0:\n",
    "            \n",
    "            # Adding a column that would be used to perform short trading (sets 1 to 1 and 0 to -1)\n",
    "            dayDf['LongShort'] = dayDf['predClass'].apply(lambda x: 1 if x == 1 else -1)           \n",
    "            \n",
    "            # If slippage is used then a small random factor is added or subtracted to the prices\n",
    "            if useSlippage:\n",
    "                dayDf['tradeactualPrice'] = dayDf['tradeactualPrice'] + np.random.uniform(low=-slippageChange, high=slippageChange, size=len(dayDf))\n",
    "                dayDf['actualPrice'] = dayDf['actualPrice'] + np.random.uniform(low=-slippageChange, high=slippageChange, size=len(dayDf))\n",
    "            \n",
    "            # Calculating the percentage change\n",
    "            dayDf['pct_change'] = dayDf[['tradeactualPrice', 'actualPrice']].pct_change(axis=1)['actualPrice']\n",
    "            \n",
    "            # Calculating how the capital will be divided\n",
    "            investmentPerStock = portfolio_value/dayDf.shape[0]\n",
    "            investmentPerStock = np.ones(dayDf.shape[0])*investmentPerStock\n",
    "\n",
    "            # If certainty is used then the investment size is varied \n",
    "            if useCertainty:\n",
    "                investmentPerStock = investmentPerStock*dayDf['certainty'] # dayDf['certainty'] ranges from 0 to 1\n",
    "                remainingLiquid = portfolio_value - np.sum(investmentPerStock) # Noting what was not invested\n",
    "            \n",
    "            # Calculating the profit or loss that was made on the trades\n",
    "            dayDf['profitLoss'] = dayDf['pct_change']*investmentPerStock*dayDf['LongShort']\n",
    "            \n",
    "            # Calculating the new portfolio value\n",
    "            portfolio_value = np.sum(investmentPerStock) + np.sum(np.array(dayDf['profitLoss'])) + remainingLiquid\n",
    "\n",
    "            # Accounting for transaction fees\n",
    "            if usetransactionFee:\n",
    "                # Multiplied by two since two transactions per order are occuring, buy and sell\n",
    "                \n",
    "                fee = transactionFee*np.sum(investmentPerStock) + transactionFee*(np.sum(np.array(dayDf['profitLoss'])) + np.sum(investmentPerStock)) \n",
    "                \n",
    "                if fee/2 < 2: # minimum fee is 2$\n",
    "                    fee = 2*2 # $2 * 2 orders\n",
    "                \n",
    "                portfolio_value -= fee\n",
    "                \n",
    "    \n",
    "    # Storing the data in a df and returning it\n",
    "    portfolio_valueDf = pd.DataFrame(portfolio_value_list)\n",
    "    portfolio_valueDf.columns = ['Date', 'portfolio_value']\n",
    "    \n",
    "    #print('Debugging.... <><><><><><><><><><><><><><><><>')\n",
    "    #print(df[0:100])\n",
    "    \n",
    "    #print(portfolio_valueDf[0:100])\n",
    "    #print('END Debugging.... <><><><><><><><><><><><><><><><>')\n",
    "    \n",
    "    return portfolio_valueDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3547cfcd",
   "metadata": {},
   "source": [
    "def tradingStrategy(df,portfolio_value = 1000, useCertainty = True, usetransactionFee = True, transactionFee = 0.005, useSlippage = True, slippageChange = 0.1, certaintyThreshold = 0.1, short=False, skip=False):\n",
    "    \n",
    "    # Extracting the unique trading dates\n",
    "    tradingDates = df.tradedate.unique()\n",
    "    \n",
    "    portfolio_value_list = list()\n",
    "    remainingLiquid = 0 \n",
    "    \n",
    "    # Trading on each trading day\n",
    "    for tradingDate in tradingDates:\n",
    "        \n",
    "        # Storing the data that will later be used for evaluation\n",
    "        portfolio_value_list.append([tradingDate, portfolio_value])\n",
    "        \n",
    "        if skip:\n",
    "            # If skip signal is 0 this means that trade normal\n",
    "            if df.query('tradedate==\"%s\"' % (tradingDate))['skip'].values[0] == 0:\n",
    "                # Filtering the datafram based on whether short positons wil be considered or not (if yes Sell signals are not neglected)\n",
    "                if short:\n",
    "                    dayDf = df.query('tradedate==\"%s\" & (predClass==1 | predClass==0) & certainty < %f' % (tradingDate, float(certaintyThreshold)))\n",
    "                else:\n",
    "                    dayDf = df.query('tradedate==\"%s\" & predClass==1 & certainty < %f' % (tradingDate, float(certaintyThreshold)))\n",
    "            else: #If skip = 1, then no trades shall occur. Hence create a dataframe with no trades\n",
    "                dayDf = pd.DataFrame(columns=['index', 'tradedate', 'date', 'tradeactualPrice', 'actualPrice', 'actualClass', 'predClass', 'certainty', 'skip'])\n",
    "        else:\n",
    "            if short:\n",
    "                dayDf = df.query('tradedate==\"%s\" & (predClass==1 | predClass==0) & certainty < %f' % (tradingDate, float(certaintyThreshold)))\n",
    "            else:\n",
    "                dayDf = df.query('tradedate==\"%s\" & predClass==1 & certainty < %f' % (tradingDate, float(certaintyThreshold)))\n",
    "        \n",
    "        # Check whether there a profitable at the current trading day\n",
    "        if dayDf.shape[0] > 0:\n",
    "            \n",
    "            # Adding a column that would be used to perform short trading (sets 1 to 1 and 0 to -1)\n",
    "            dayDf['LongShort'] = dayDf['predClass'].apply(lambda x: 1 if x == 1 else -1)           \n",
    "            \n",
    "            # If slippage is used then a small random factor is added or subtracted to the prices\n",
    "            if useSlippage:\n",
    "                dayDf['tradeactualPrice'] = dayDf['tradeactualPrice'] + np.random.uniform(low=-slippageChange, high=slippageChange, size=len(dayDf))\n",
    "                dayDf['actualPrice'] = dayDf['actualPrice'] + np.random.uniform(low=-slippageChange, high=slippageChange, size=len(dayDf))\n",
    "            \n",
    "            # Calculating the percentage change\n",
    "            dayDf['pct_change'] = dayDf[['tradeactualPrice', 'actualPrice']].pct_change(axis=1)['actualPrice']\n",
    "            \n",
    "            # Calculating how the capital will be divided\n",
    "            investmentPerStock = portfolio_value/dayDf.shape[0]\n",
    "            investmentPerStock = np.ones(dayDf.shape[0])*investmentPerStock\n",
    "\n",
    "            # If certainty is used then the investment size is varied \n",
    "            if useCertainty:\n",
    "                investmentPerStock = investmentPerStock*dayDf['certainty'] # dayDf['certainty'] ranges from 0 to 1\n",
    "                remainingLiquid = portfolio_value - np.sum(investmentPerStock) # Noting what was not invested\n",
    "            \n",
    "            # Calculating the profit or loss that was made on the trades\n",
    "            dayDf['profitLoss'] = dayDf['pct_change']*investmentPerStock*dayDf['LongShort']\n",
    "            \n",
    "            # Calculating the new portfolio value\n",
    "            portfolio_value = np.sum(investmentPerStock) + np.sum(np.array(dayDf['profitLoss'])) + remainingLiquid\n",
    "\n",
    "            # Accounting for transaction fees\n",
    "            if usetransactionFee:\n",
    "                # Multiplied by two since two transactions per order are occuring, buy and sell\n",
    "                portfolio_value -= transactionFee*dayDf.shape[0]*2\n",
    "    \n",
    "    # Storing the data in a df and returning it\n",
    "    portfolio_valueDf = pd.DataFrame(portfolio_value_list)\n",
    "    portfolio_valueDf.columns = ['Date', 'portfolio_value']\n",
    "    \n",
    "    #print('Debugging.... <><><><><><><><><><><><><><><><>')\n",
    "    #print(df[0:100])\n",
    "    \n",
    "    #print(portfolio_valueDf[0:100])\n",
    "    #print('END Debugging.... <><><><><><><><><><><><><><><><>')\n",
    "    \n",
    "    return portfolio_valueDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e54d24",
   "metadata": {},
   "source": [
    "The below function works CORRECTLY but was updated under it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d045b3",
   "metadata": {},
   "source": [
    "Below code was updated such that the when using enbpi, the trading when not using uncertainty is done using the ensemble model predictions but the original model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedda057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trade_on_test(df, initialCapital, folder_name, df_og_data=None):\n",
    "    \n",
    "    df.to_csv(os.path.join(folder_name, \"trading_data.csv\"), index=False)\n",
    "    \n",
    "    # Hard-coding the settings for the trading strategy variations\n",
    "    settings = [[False, 100, False], [True, 100, False],[False, 100, True], [True, 100, True]]\n",
    "    description = ['Long','Long using thresholded certainty','Short','Short using thresholded certainty']\n",
    "\n",
    "    for ind, setting in enumerate(settings):\n",
    "\n",
    "        # Create an empty DataFrame to store the metrics\n",
    "        metrics_df = pd.DataFrame(columns=['Metric', 'Mean', 'Standard Deviation'])\n",
    "\n",
    "        # Run the function multiple times and collect the metrics\n",
    "        num_runs = 1\n",
    "        results = []\n",
    "\n",
    "        for i in range(num_runs):\n",
    "\n",
    "            #df_mod = skip_day(df, tuned_threshold, days=4)\n",
    "            \n",
    "            if setting == 'Long' and df_og_data != None or setting == 'Short' and df_og_data != None:\n",
    "                \n",
    "                portfolio_valueDf = tradingStrategy(df_og_data,\n",
    "                                               portfolio_value = initialCapital, \n",
    "                                               useCertainty = setting[0], \n",
    "                                               usetransactionFee = True, \n",
    "                                               transactionFee = 0.0001*0.2, #fee from IBRKs\n",
    "                                               useSlippage = False, \n",
    "                                               slippageChange = 0.1, \n",
    "                                               certaintyThreshold = setting[1],\n",
    "                                               short = setting[2],\n",
    "                                               skip = False)\n",
    "            else:\n",
    "                \n",
    "                portfolio_valueDf = tradingStrategy(df,\n",
    "                                                   portfolio_value = initialCapital, \n",
    "                                                   useCertainty = setting[0], \n",
    "                                                   usetransactionFee = True, \n",
    "                                                   transactionFee = 0.0001*0.2, #fee from IBRKs\n",
    "                                                   useSlippage = False, \n",
    "                                                   slippageChange = 0.1, \n",
    "                                                   certaintyThreshold = setting[1],\n",
    "                                                   short = setting[2],\n",
    "                                                   skip = False)\n",
    "\n",
    "            metrics = calculate_portfolio_metrics(portfolio_valueDf, plot=False, og_data=df)\n",
    "            results.append(metrics)\n",
    "        \n",
    "        portfolio_valueDf.to_csv(os.path.join(folder_name, f\"portfolio_valueDf_{description[ind].replace(' ', '')}.csv\"), index=False)\n",
    "\n",
    "        # Calculating the mean and variance of the metrics\n",
    "        metrics_mean = np.mean(results, axis=0)\n",
    "        metrics_variance = np.std(results, axis=0)\n",
    "\n",
    "        # Storing the data\n",
    "        #metrics_df['Metric'] = ['Cumulative Returns', 'Annualized Volatility', 'Sharpe Ratio',\n",
    "        #                        'Sortino Ratio', 'Beta','Alpha', 'Maximum Drawdown']\n",
    "\n",
    "        metrics_df['Metric'] = ['Cumulative Returns', 'Annualized Volatility', 'Sharpe Ratio',\n",
    "                                'Sortino Ratio', 'Maximum Drawdown','beta', 'alpha']\n",
    "\n",
    "        metrics_df['Mean'] = metrics_mean\n",
    "        metrics_df['Standard Deviation'] = metrics_variance\n",
    "\n",
    "        # Converting the data to 3 decimal places\n",
    "        metrics_df['Mean'] = metrics_df['Mean'].apply(lambda x: f'{x:.3e}')\n",
    "        metrics_df['Standard Deviation'] = metrics_df['Standard Deviation'].apply(lambda x: f'{x:.3e}')\n",
    "\n",
    "        # Printing the metrics\n",
    "        print(description[ind])\n",
    "        print(metrics_df)\n",
    "        print('\\n')\n",
    "\n",
    "        # Saving the metrics to a csv file\n",
    "        metrics_df.to_csv(os.path.join(folder_name, f\"{'test_portfolio_metrics'+str(ind)}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1a79b1",
   "metadata": {},
   "source": [
    "def trade_on_test(df, initialCapital, folder_name):\n",
    "    \n",
    "    df.to_csv(os.path.join(folder_name, \"trading_data.csv\"), index=False)\n",
    "    \n",
    "    # Hard-coding the settings for the trading strategy variations\n",
    "    settings = [[False, 100, False], [True, 100, False],[False, 100, True], [True, 100, True]]\n",
    "    description = ['Long','Long using thresholded certainty','Short','Short using thresholded certainty']\n",
    "\n",
    "    for ind, setting in enumerate(settings):\n",
    "\n",
    "        # Create an empty DataFrame to store the metrics\n",
    "        metrics_df = pd.DataFrame(columns=['Metric', 'Mean', 'Standard Deviation'])\n",
    "\n",
    "        # Run the function multiple times and collect the metrics\n",
    "        num_runs = 1\n",
    "        results = []\n",
    "\n",
    "        for i in range(num_runs):\n",
    "\n",
    "            #df_mod = skip_day(df, tuned_threshold, days=4)\n",
    "\n",
    "            portfolio_valueDf = tradingStrategy(df,\n",
    "                                               portfolio_value = initialCapital, \n",
    "                                               useCertainty = setting[0], \n",
    "                                               usetransactionFee = True, \n",
    "                                               transactionFee = 0.0001*0.2, #fee from IBRKs\n",
    "                                               useSlippage = False, \n",
    "                                               slippageChange = 0.1, \n",
    "                                               certaintyThreshold = setting[1],\n",
    "                                               short = setting[2],\n",
    "                                               skip = False)\n",
    "\n",
    "            metrics = calculate_portfolio_metrics(portfolio_valueDf, plot=False, og_data=df)\n",
    "            results.append(metrics)\n",
    "        \n",
    "        portfolio_valueDf.to_csv(os.path.join(folder_name, f\"portfolio_valueDf_{description[ind].replace(' ', '')}.csv\"), index=False)\n",
    "\n",
    "        # Calculating the mean and variance of the metrics\n",
    "        metrics_mean = np.mean(results, axis=0)\n",
    "        metrics_variance = np.std(results, axis=0)\n",
    "\n",
    "        # Storing the data\n",
    "        #metrics_df['Metric'] = ['Cumulative Returns', 'Annualized Volatility', 'Sharpe Ratio',\n",
    "        #                        'Sortino Ratio', 'Beta','Alpha', 'Maximum Drawdown']\n",
    "\n",
    "        metrics_df['Metric'] = ['Cumulative Returns', 'Annualized Volatility', 'Sharpe Ratio',\n",
    "                                'Sortino Ratio', 'Maximum Drawdown','beta', 'alpha']\n",
    "\n",
    "        metrics_df['Mean'] = metrics_mean\n",
    "        metrics_df['Standard Deviation'] = metrics_variance\n",
    "\n",
    "        # Converting the data to 3 decimal places\n",
    "        metrics_df['Mean'] = metrics_df['Mean'].apply(lambda x: f'{x:.3e}')\n",
    "        metrics_df['Standard Deviation'] = metrics_df['Standard Deviation'].apply(lambda x: f'{x:.3e}')\n",
    "\n",
    "        # Printing the metrics\n",
    "        print(description[ind])\n",
    "        print(metrics_df)\n",
    "        print('\\n')\n",
    "\n",
    "        # Saving the metrics to a csv file\n",
    "        metrics_df.to_csv(os.path.join(folder_name, f\"{'test_portfolio_metrics'+str(ind)}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380d3c9a",
   "metadata": {},
   "source": [
    "*Function that finds the best threshold based on the validation data*\n",
    "\n",
    "*Three options to optimise - Directional Accuracy, Profit (using thresholded uncertainty), Sharpe (using thresholded uncertainty)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0ec86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to add another metric... Directional Accuracy using the df itself, threshold using query and see when direction was correct\n",
    "# Pass the threshold to the next section\n",
    "\n",
    "# ADD: Consider both long and short \n",
    "\n",
    "def tune_threshold(val_df, initialCapital, folder_name, useCertainty=True, short=False, optimise='profit', days=4, test=False):\n",
    "    \n",
    "    #step = 0.05\n",
    "    #certaintyThresholds = np.arange(0,1+step,step)\n",
    "    \n",
    "    # !!WHEN USING UN-NORMALISED UNCERTAINTY!!\n",
    "    step = (val_df['certainty'].max() - val_df['certainty'].min())/20\n",
    "    certaintyThresholds = np.arange(val_df['certainty'].min(),val_df['certainty'].max()+step,step)\n",
    "    \n",
    "    results, correctDADFlens, certainpredsDFlens, percentDAlist = [], [], [], []\n",
    "    \n",
    "    for certaintyThreshold in certaintyThresholds:\n",
    "        \n",
    "        val_df_mod = skip_day(val_df, certaintyThreshold, days=days)\n",
    "        \n",
    "        # Generating a portfolio from the validation data\n",
    "        portfolio_valueDf = tradingStrategy(val_df_mod,\n",
    "                                           portfolio_value = initialCapital, \n",
    "                                           useCertainty = useCertainty, \n",
    "                                           usetransactionFee = False, \n",
    "                                           transactionFee = 0.0001*0.2, #fee from IBRKs \n",
    "                                           useSlippage = False, \n",
    "                                           slippageChange = 0.1, \n",
    "                                           certaintyThreshold = certaintyThreshold,\n",
    "                                           short = short,\n",
    "                                           skip = False)\n",
    "\n",
    "        # Calculating the portfolio metrics\n",
    "        # ['Cumulative Returns', 'Annualized Volatility', 'Sharpe Ratio', 'Sortino Ratio', 'Maximum Drawdown']\n",
    "        metrics = calculate_portfolio_metrics(portfolio_valueDf, plot=False)\n",
    "        results.append(metrics)\n",
    "        \n",
    "        # Number of correct predictions under the threshold level\n",
    "        correctDADF = val_df_mod.query('((predClass==1 & actualClass==1) | (predClass==0 & actualClass==0)) & certainty < %f & skip == 0' % (float(certaintyThreshold)))\n",
    "        correctDADFlens.append(len(correctDADF.index))\n",
    "        \n",
    "        # Number of predictions under the threshold level\n",
    "        certainpredsDF = val_df_mod.query('certainty < %f & skip == 0' % (float(certaintyThreshold)))\n",
    "        certainpredsDFlens.append(len(certainpredsDF.index))\n",
    "        \n",
    "        #if certaintyThreshold == 0.1:\n",
    "            #val_df_mod.to_csv('val_df_mod.csv')\n",
    "        \n",
    "        if len(certainpredsDF.index) != 0:\n",
    "            percentDAlist.append((len(correctDADF.index)/len(certainpredsDF.index))*100)\n",
    "        else:\n",
    "            percentDAlist.append(np.nan)\n",
    "    \n",
    "    results_df = pd.DataFrame(data=results, columns=['CumulativeReturns', 'Annualized Volatility', 'SharpeRatio', 'Sortino Ratio', 'Maximum Drawdown', 'beta', 'alpha'])\n",
    "    \n",
    "    results_df.insert(0, \"threshold\", certaintyThresholds)\n",
    "    results_df.insert(0, \"correctpredictions\", correctDADFlens)\n",
    "    results_df.insert(0, \"numberofpredictions\", certainpredsDFlens)\n",
    "    results_df.insert(0, \"percentDA\", percentDAlist)\n",
    "    \n",
    "    \n",
    "    # Plotting the uncertainty\n",
    "    \n",
    "    plt.plot(val_df['tradedate'], val_df['certainty'])\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Certainty')\n",
    "    plt.grid()\n",
    "    if not test:\n",
    "        plot_name = 'Certaintyvstradedate_days'+str(days)+'_optimise'+optimise+'_short'+str(short)\n",
    "        plt.title('Certainty Plot (Validation Data)')\n",
    "    else:\n",
    "        plot_name = 'TEST_Certaintyvstradedate_days'+str(days)+'_optimise'+optimise+'_short'+str(short)\n",
    "        plt.title('Certainty Plot (Test Data)')\n",
    "    plt.savefig(os.path.join(folder_name, plot_name))\n",
    "    plt.show()\n",
    "    \n",
    "    # Plotting the da\n",
    "    \n",
    "    plt.plot(results_df['threshold'], results_df['percentDA'])\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('DA (%)')\n",
    "    plt.grid()\n",
    "    if not test:\n",
    "        plot_name = 'DAvsThreshold_days'+str(days)+'_optimise'+optimise+'_short'+str(short)\n",
    "        plt.title('DA vs Threshold (Validation Data)')\n",
    "    else:\n",
    "        plot_name = 'TEST_DAvsThreshold_days'+str(days)+'_optimise'+optimise+'_short'+str(short)\n",
    "        plt.title('DA vs Threshold (Test Data)')\n",
    "    plt.savefig(os.path.join(folder_name, plot_name))\n",
    "    plt.show()\n",
    "    \n",
    "    # Plotting the SharpeRatio\n",
    "    \n",
    "    plt.plot(results_df['threshold'], results_df['SharpeRatio'])\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Sharpe Ratio')\n",
    "    plt.grid()\n",
    "    if not test:\n",
    "        plot_name = 'SharpeRatiovsThreshold_days'+str(days)+'_optimise'+optimise+'_short'+str(short)\n",
    "        plt.title('Sharpe Ratio vs Threshold (Validation Data)')\n",
    "    else:\n",
    "        plot_name = 'TEST_SharpeRatiovsThreshold_days'+str(days)+'_optimise'+optimise+'_short'+str(short)\n",
    "        plt.title('Sharpe Ratio vs Threshold (Test Data)')\n",
    "    plt.savefig(os.path.join(folder_name, plot_name))\n",
    "    plt.show()\n",
    "    \n",
    "    # Plotting the CumulativeReturns\n",
    "    \n",
    "    plt.plot(results_df['threshold'], results_df['CumulativeReturns'])\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Returns')\n",
    "    plt.grid()\n",
    "    if not test:\n",
    "        plot_name = 'CumulativeReturnsvsThreshold_days'+str(days)+'_optimise'+optimise+'_short'+str(short)\n",
    "        plt.title('Cumulative Returns vs Threshold (Validation Data)')\n",
    "    else:\n",
    "        plot_name = 'TEST_CumulativeReturnsvsThreshold_days'+str(days)+'_optimise'+optimise+'_short'+str(short)\n",
    "        plt.title('Cumulative Returns vs Threshold (Test Data)')\n",
    "    plt.savefig(os.path.join(folder_name, plot_name))\n",
    "    plt.show()\n",
    "    \n",
    "    if not test:\n",
    "        results_df.to_csv(os.path.join(folder_name, f\"{'Tuning_Outcome'}.csv\"), index=False)\n",
    "    else:\n",
    "        results_df.to_csv(os.path.join(folder_name, f\"{'TEST_Tuning_Outcome'}.csv\"), index=False)\n",
    "        \n",
    "    # Finding the best threshold for the required optimisation\n",
    "    if optimise == 'profit':\n",
    "        profitOptimisedPortfolio = results_df[results_df.CumulativeReturns == results_df.CumulativeReturns.max()]\n",
    "        print('Profit Optimised Portfolio')\n",
    "        print(profitOptimisedPortfolio)\n",
    "        tuned_threshold = profitOptimisedPortfolio['threshold'].values[0]\n",
    "        tuned_parameter = profitOptimisedPortfolio['CumulativeReturns'].values[0]\n",
    "    elif optimise == 'sharpe':\n",
    "        sharpeOptimisedPortfolio = results_df[results_df.SharpeRatio == results_df.SharpeRatio.max()]\n",
    "        print('Sharpe Ratio Optimised Portfolio')\n",
    "        print(sharpeOptimisedPortfolio)\n",
    "        tuned_threshold = sharpeOptimisedPortfolio['threshold'].values[0]\n",
    "        tuned_parameter = sharpeOptimisedPortfolio['SharpeRatio'].values[0]\n",
    "    elif optimise == 'da':\n",
    "        DAOptimisedPortfolio = results_df[results_df.percentDA == results_df.percentDA.max()]\n",
    "        print('Directional Accuracy Optimised Portfolio')\n",
    "        print(DAOptimisedPortfolio)\n",
    "        tuned_threshold = DAOptimisedPortfolio['threshold'].values[0]\n",
    "        tuned_parameter = DAOptimisedPortfolio['percentDA'].values[0]\n",
    "        \n",
    "    tuned_thres = {'tuned_threshold':[tuned_threshold],'tuned_parameter':tuned_parameter}\n",
    "    tuned_df = pd.DataFrame(tuned_thres)\n",
    "    \n",
    "    if not test:\n",
    "        tuned_df.to_csv(os.path.join(folder_name, f\"{'Tuned_Threshold_optimise'+optimise}.csv\"), index=False)\n",
    "    else:\n",
    "        tuned_df.to_csv(os.path.join(folder_name, f\"{'TEST_Tuned_Threshold_optimise'+optimise}.csv\"), index=False)\n",
    "    \n",
    "    return tuned_threshold\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13fba2b",
   "metadata": {},
   "source": [
    "*Testing all the possible trading methods*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65424701",
   "metadata": {},
   "source": [
    "def tradingStrategy(df,portfolio_value = 1000, useCertainty = True, usetransactionFee = True, transactionFee = 0.005, useSlippage = True, slippageChange = 0.1, certaintyThreshold = 0.1, short=False, skip=True):\n",
    "    \n",
    "    # Extracting the unique trading dates\n",
    "    tradingDates = df.tradedate.unique()\n",
    "    \n",
    "    portfolio_value_list = list()\n",
    "    remainingLiquid = 0 \n",
    "    \n",
    "    # Trading on each trading day\n",
    "    for tradingDate in tradingDates:\n",
    "        \n",
    "        # Storing the data that will later be used for evaluation\n",
    "        portfolio_value_list.append([tradingDate, portfolio_value])\n",
    "        \n",
    "        if skip:\n",
    "            # If skip signal is 0 this means that trade normal\n",
    "            if df.query('tradedate==\"%s\"' % (tradingDate))['skip'].values[0] == 0:\n",
    "                # Filtering the datafram based on whether short positons wil be considered or not (if yes Sell signals are not neglected)\n",
    "                if short:\n",
    "                    dayDf = df.query('tradedate==\"%s\" & (predClass==1 | predClass==0) & certainty < %f' % (tradingDate, float(certaintyThreshold)))\n",
    "                else:\n",
    "                    dayDf = df.query('tradedate==\"%s\" & predClass==1 & certainty < %f' % (tradingDate, float(certaintyThreshold)))\n",
    "            else: #If skip = 1, then no trades shall occur. Hence create a dataframe with no trades\n",
    "                dayDf = pd.DataFrame(columns=['index', 'tradedate', 'date', 'tradeactualPrice', 'actualPrice', 'actualClass', 'predClass', 'certainty', 'skip'])\n",
    "        else:\n",
    "            if short:\n",
    "                dayDf = df.query('tradedate==\"%s\" & (predClass==1 | predClass==0) & certainty < %f' % (tradingDate, float(certaintyThreshold)))\n",
    "            else:\n",
    "                dayDf = df.query('tradedate==\"%s\" & predClass==1 & certainty < %f' % (tradingDate, float(certaintyThreshold)))\n",
    "        \n",
    "        # Check whether there a profitable at the current trading day\n",
    "        if dayDf.shape[0] > 0:\n",
    "            \n",
    "            # Adding a column that would be used to perform short trading (sets 1 to 1 and 0 to -1)\n",
    "            dayDf['LongShort'] = dayDf['predClass'].apply(lambda x: 1 if x == 1 else -1)           \n",
    "            \n",
    "            # If slippage is used then a small random factor is added or subtracted to the prices\n",
    "            if useSlippage:\n",
    "                dayDf['tradeactualPrice'] = dayDf['tradeactualPrice'] + np.random.uniform(low=-slippageChange, high=slippageChange, size=len(dayDf))\n",
    "                dayDf['actualPrice'] = dayDf['actualPrice'] + np.random.uniform(low=-slippageChange, high=slippageChange, size=len(dayDf))\n",
    "            \n",
    "            # Calculating the percentage change\n",
    "            dayDf['pct_change'] = dayDf[['tradeactualPrice', 'actualPrice']].pct_change(axis=1)['actualPrice']\n",
    "            \n",
    "            # Calculating how the capital will be divided\n",
    "            investmentPerStock = portfolio_value/dayDf.shape[0]\n",
    "            investmentPerStock = np.ones(dayDf.shape[0])*investmentPerStock\n",
    "\n",
    "            # If certainty is used then the investment size is varied \n",
    "            if useCertainty:\n",
    "                investmentPerStock = investmentPerStock*dayDf['certainty'] # dayDf['certainty'] ranges from 0 to 1\n",
    "                remainingLiquid = portfolio_value - np.sum(investmentPerStock) # Noting what was not invested\n",
    "            \n",
    "            # Calculating the profit or loss that was made on the trades\n",
    "            dayDf['profitLoss'] = dayDf['pct_change']*investmentPerStock*dayDf['LongShort']\n",
    "            \n",
    "            # Calculating the new portfolio value\n",
    "            portfolio_value = np.sum(investmentPerStock) + np.sum(np.array(dayDf['profitLoss'])) + remainingLiquid\n",
    "\n",
    "            # Accounting for transaction fees\n",
    "            if usetransactionFee:\n",
    "                # Multiplied by two since two transactions per order are occuring, buy and sell\n",
    "                portfolio_value -= transactionFee*dayDf.shape[0]*2\n",
    "    \n",
    "    # Storing the data in a df and returning it\n",
    "    portfolio_valueDf = pd.DataFrame(portfolio_value_list)\n",
    "    portfolio_valueDf.columns = ['Date', 'portfolio_value']\n",
    "    return portfolio_valueDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18c7d3a",
   "metadata": {},
   "source": [
    "def trade_on_test(df, tuned_threshold, initialCapital, folder_name):\n",
    "        \n",
    "    # Hard-coding the settings for the trading strategy variations\n",
    "    settings = [[False, 100, False],[True, 100, False], [True, tuned_threshold, False],[False, 100, True],[True, 100, True], [True, tuned_threshold, True]]\n",
    "    description = ['Long','Long using certainty','Long using thresholded certainty','Short','Short using certainty','Short using thresholded certainty']\n",
    "\n",
    "    for ind, setting in enumerate(settings):\n",
    "\n",
    "        # Create an empty DataFrame to store the metrics\n",
    "        metrics_df = pd.DataFrame(columns=['Metric', 'Mean', 'Standard Deviation'])\n",
    "\n",
    "        # Run the function multiple times and collect the metrics\n",
    "        num_runs = 1\n",
    "        results = []\n",
    "\n",
    "        for i in range(num_runs):\n",
    "\n",
    "            df_mod = skip_day(df, tuned_threshold, days=4)\n",
    "\n",
    "            portfolio_valueDf = tradingStrategy(df_mod,\n",
    "                                               portfolio_value = initialCapital, \n",
    "                                               useCertainty = setting[0], \n",
    "                                               usetransactionFee = True, \n",
    "                                               transactionFee = 0.005, \n",
    "                                               useSlippage = False, \n",
    "                                               slippageChange = 0.1, \n",
    "                                               certaintyThreshold = setting[1],\n",
    "                                               short = setting[2],\n",
    "                                               skip = True)\n",
    "\n",
    "            metrics = calculate_portfolio_metrics(portfolio_valueDf, plot=False)\n",
    "            results.append(metrics)\n",
    "        \n",
    "        portfolio_valueDf.to_csv(os.path.join(folder_name, f\"portfolio_valueDf_{description[ind].replace(' ', '')}.csv\"), index=False)\n",
    "\n",
    "        # Calculating the mean and variance of the metrics\n",
    "        metrics_mean = np.mean(results, axis=0)\n",
    "        metrics_variance = np.std(results, axis=0)\n",
    "\n",
    "        # Storing the data\n",
    "        #metrics_df['Metric'] = ['Cumulative Returns', 'Annualized Volatility', 'Sharpe Ratio',\n",
    "        #                        'Sortino Ratio', 'Beta','Alpha', 'Maximum Drawdown']\n",
    "\n",
    "        metrics_df['Metric'] = ['Cumulative Returns', 'Annualized Volatility', 'Sharpe Ratio',\n",
    "                                'Sortino Ratio', 'Maximum Drawdown']\n",
    "\n",
    "        metrics_df['Mean'] = metrics_mean\n",
    "        metrics_df['Standard Deviation'] = metrics_variance\n",
    "\n",
    "        # Converting the data to 3 decimal places\n",
    "        metrics_df['Mean'] = metrics_df['Mean'].apply(lambda x: f'{x:.3e}')\n",
    "        metrics_df['Standard Deviation'] = metrics_df['Standard Deviation'].apply(lambda x: f'{x:.3e}')\n",
    "\n",
    "        # Printing the metrics\n",
    "        print(description[ind])\n",
    "        print(metrics_df)\n",
    "        print('\\n')\n",
    "\n",
    "        # Saving the metrics to a csv file\n",
    "        metrics_df.to_csv(os.path.join(folder_name, f\"{'test_portfolio_metrics'+str(ind)}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf73f7",
   "metadata": {},
   "source": [
    "*Plotting the result*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a621e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_equity_curve(df, initialCapital, folder_name):\n",
    "    \n",
    "    settings = [[False, 100, False], [True, 100, False],[False, 100, True], [True, 100, True]]\n",
    "    description = ['Long','Long using thresholded certainty','Short','Short using thresholded certainty']\n",
    "    \n",
    "    sns.set(style='whitegrid')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # For all the trading strategy variations\n",
    "    for setting in settings:\n",
    "\n",
    "        portfolio_valueDf = tradingStrategy(df,\n",
    "                                               portfolio_value = initialCapital, \n",
    "                                               useCertainty = setting[0], \n",
    "                                               usetransactionFee = True, \n",
    "                                               transactionFee = 0.0001*0.2, #fee from IBRKs \n",
    "                                               useSlippage = False, \n",
    "                                               slippageChange = 0.1, \n",
    "                                               certaintyThreshold = setting[1],\n",
    "                                               short = setting[2],\n",
    "                                               skip = False)\n",
    "        # Plotting portfolio value\n",
    "        portfolio_valueDf['Date'] = pd.to_datetime(portfolio_valueDf['Date'])\n",
    "        plt.plot(portfolio_valueDf['Date'], portfolio_valueDf['portfolio_value'], linewidth=2)\n",
    "\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Portfolio Value ($)')\n",
    "    plt.title('Portfolio Growth Using an Different Trading Techniques')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.xlim(portfolio_valueDf['Date'].iloc[0], portfolio_valueDf['Date'].iloc[-1])  \n",
    "    plt.legend(['Long','Long using thresholded certainty','Short','Short using thresholded certainty'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    sns.despine()\n",
    "    plot_name = \"portfolio_growth_with_all_strategies.png\"\n",
    "    plt.savefig(os.path.join(folder_name, plot_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b1fd31",
   "metadata": {},
   "source": [
    "def plot_equity_curve(df, tuned_threshold, initialCapital, folder_name):\n",
    "    \n",
    "    df_mod = skip_day(df, tuned_threshold, days=4)\n",
    "    \n",
    "    # Calculating the DA on the test data\n",
    "    # Number of correct predictions under the threshold level\n",
    "    correctDADF = df_mod.query('((predClass==1 & actualClass==1) | (predClass==0 & actualClass==0)) & certainty < %f & skip == 0' % (float(tuned_threshold)))\n",
    "\n",
    "    # Number of predictions under the threshold level\n",
    "    certainpredsDF = df_mod.query('certainty < %f & skip == 0' % (float(tuned_threshold)))\n",
    "    \n",
    "    if len(certainpredsDF.index) != 0:\n",
    "        percent_da=(len(correctDADF.index)/len(certainpredsDF.index))*100\n",
    "    else:\n",
    "        percent_da=np.nan\n",
    "    \n",
    "    percent_da = {'test_percent_da':[percent_da]}\n",
    "    percent_da_df = pd.DataFrame(percent_da)\n",
    "    percent_da_df.to_csv(os.path.join(folder_name, f\"{'Test_Percent_DA'}.csv\"), index=False)\n",
    "\n",
    "    settings = [[False, 100, False],[True, 100, False], [True, tuned_threshold, False],[False, 100, True],[True, 100, True], [True, tuned_threshold, True]]\n",
    "    description = ['Long','Long using certainty','Long using thresholded certainty','Short','Short using certainty','Short using thresholded certainty']\n",
    "    \n",
    "    sns.set(style='whitegrid')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # For all the trading strategy variations\n",
    "    for setting in settings:\n",
    "\n",
    "        portfolio_valueDf = tradingStrategy(df_mod,\n",
    "                                               portfolio_value = initialCapital, \n",
    "                                               useCertainty = setting[0], \n",
    "                                               usetransactionFee = True, \n",
    "                                               transactionFee = 0.005, \n",
    "                                               useSlippage = False, \n",
    "                                               slippageChange = 0.1, \n",
    "                                               certaintyThreshold = setting[1],\n",
    "                                               short = setting[2],\n",
    "                                               skip = True)\n",
    "        # Plotting portfolio value\n",
    "        portfolio_valueDf['Date'] = pd.to_datetime(portfolio_valueDf['Date'])\n",
    "        plt.plot(portfolio_valueDf['Date'], portfolio_valueDf['portfolio_value'], linewidth=2)\n",
    "\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Portfolio Value ($)')\n",
    "    plt.title('Portfolio Growth Using an Different Trading Techniques')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.xlim(portfolio_valueDf['Date'].iloc[0], portfolio_valueDf['Date'].iloc[-1])  \n",
    "    plt.legend(['Long','Long using certainty','Long using thresholded certainty','Short','Short using certainty','Short using thresholded certainty'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    sns.despine()\n",
    "    plot_name = \"portfolio_growth_with_all_strategies.png\"\n",
    "    plt.savefig(os.path.join(folder_name, plot_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c65996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def superimpose_tuned_data(folder_name, short, days, optimise):\n",
    "    \n",
    "    # Superimposing the validation and test data tuned threshold plots\n",
    "    val_tuning_data = read_csv_as_df(\"Tuning_Outcome.csv\",folder_name)\n",
    "    test_tuning_data = read_csv_as_df(\"TEST_Tuning_Outcome.csv\",folder_name)\n",
    "\n",
    "    # Plotting the threshold\n",
    "\n",
    "    plt.plot(val_tuning_data['threshold'], val_tuning_data['percentDA'], label='Val')\n",
    "    plt.plot(test_tuning_data['threshold'], test_tuning_data['percentDA'], label='Test')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('DA (%)')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plot_name = 'TESTVAL_DAvsThreshold_days'+str(days)+'_optimise'+optimise+'_short'+str(short)\n",
    "    plt.title('DA vs Threshold (Test & Val Data)')\n",
    "    plt.savefig(os.path.join(folder_name, plot_name))\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting the SharpeRatio\n",
    "\n",
    "    plt.plot(val_tuning_data['threshold'], val_tuning_data['SharpeRatio'], label='Val')\n",
    "    plt.plot(test_tuning_data['threshold'], test_tuning_data['SharpeRatio'], label='Test')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Sharpe Ratio')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plot_name = 'TESTVAL_SharpeRatiovsThreshold_days'+str(days)+'_optimise'+optimise+'_short'+str(short)\n",
    "    plt.title('Sharpe Ratio vs Threshold (Test & Val Data)')\n",
    "    plt.savefig(os.path.join(folder_name, plot_name))\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting the CumulativeReturns\n",
    "\n",
    "    plt.plot(val_tuning_data['threshold'], val_tuning_data['CumulativeReturns'], label='Val')\n",
    "    plt.plot(test_tuning_data['threshold'], test_tuning_data['CumulativeReturns'], label='Test')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Returns')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plot_name = 'TESTVAL_CumulativeReturnsvsThreshold_days'+str(days)+'_optimise'+optimise+'_short'+str(short)\n",
    "    plt.title('Cumulative Returns vs Threshold (Test & Val Data)')\n",
    "    plt.savefig(os.path.join(folder_name, plot_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0476bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_data(folder_name, optimise, days, short, s):\n",
    "    \n",
    "    # 'folder_name', 'optimise','val_thres', 'test_thres', 'days', 'short', 's', 'optimised_value_on_val', 'optimised_value_on_test', 'test_percent_da', 'Long -> Cumulative Returns -> Mean', 'Long -> Cumulative Returns -> Std', 'Long -> Annualized Volatility -> Mean', 'Long -> Annualized Volatility -> Std', 'Long -> Sharpe Ratio -> Mean', 'Long -> Sharpe Ratio -> Std', 'Long -> Sortino Ratio -> Mean', 'Long -> Sortino Ratio -> Std', 'Long -> Maximum Drawdown -> Mean', 'Long -> Maximum Drawdown -> Std', 'Long using certainty -> Cumulative Returns -> Mean', 'Long using certainty -> Cumulative Returns -> Std', 'Long using certainty -> Annualized Volatility -> Mean', 'Long using certainty -> Annualized Volatility -> Std', 'Long using certainty -> Sharpe Ratio -> Mean', 'Long using certainty -> Sharpe Ratio -> Std', 'Long using certainty -> Sortino Ratio -> Mean', 'Long using certainty -> Sortino Ratio -> Std', 'Long using certainty -> Maximum Drawdown -> Mean', 'Long using certainty -> Maximum Drawdown -> Std', 'Long using thresholded certainty -> Cumulative Returns -> Mean', 'Long using thresholded certainty -> Cumulative Returns -> Std', 'Long using thresholded certainty -> Annualized Volatility -> Mean', 'Long using thresholded certainty -> Annualized Volatility -> Std', 'Long using thresholded certainty -> Sharpe Ratio -> Mean', 'Long using thresholded certainty -> Sharpe Ratio -> Std', 'Long using thresholded certainty -> Sortino Ratio -> Mean', 'Long using thresholded certainty -> Sortino Ratio -> Std', 'Long using thresholded certainty -> Maximum Drawdown -> Mean', 'Long using thresholded certainty -> Maximum Drawdown -> Std', 'Short -> Cumulative Returns -> Mean', 'Short -> Cumulative Returns -> Std', 'Short -> Annualized Volatility -> Mean', 'Short -> Annualized Volatility -> Std', 'Short -> Sharpe Ratio -> Mean', 'Short -> Sharpe Ratio -> Std', 'Short -> Sortino Ratio -> Mean', 'Short -> Sortino Ratio -> Std', 'Short -> Maximum Drawdown -> Mean', 'Short -> Maximum Drawdown -> Std', 'Short using certainty -> Cumulative Returns -> Mean', 'Short using certainty -> Cumulative Returns -> Std', 'Short using certainty -> Annualized Volatility -> Mean', 'Short using certainty -> Annualized Volatility -> Std', 'Short using certainty -> Sharpe Ratio -> Mean', 'Short using certainty -> Sharpe Ratio -> Std', 'Short using certainty -> Sortino Ratio -> Mean', 'Short using certainty -> Sortino Ratio -> Std', 'Short using certainty -> Maximum Drawdown -> Mean', 'Short using certainty -> Maximum Drawdown -> Std', 'Short using thresholded certainty -> Cumulative Returns -> Mean', 'Short using thresholded certainty -> Cumulative Returns -> Std', 'Short using thresholded certainty -> Annualized Volatility -> Mean', 'Short using thresholded certainty -> Annualized Volatility -> Std', 'Short using thresholded certainty -> Sharpe Ratio -> Mean', 'Short using thresholded certainty -> Sharpe Ratio -> Std', 'Short using thresholded certainty -> Sortino Ratio -> Mean', 'Short using thresholded certainty -> Sortino Ratio -> Std', 'Short using thresholded certainty -> Maximum Drawdown -> Mean', 'Short using thresholded certainty -> Maximum Drawdown -> Std'\n",
    "    \n",
    "    data = list()\n",
    "    \n",
    "    data.append(folder_name)\n",
    "    \n",
    "    data.append(optimise)\n",
    "    \n",
    "    data.append(days)\n",
    "    \n",
    "    data.append(short)\n",
    "    \n",
    "    data.append(s)\n",
    "    \n",
    "    for i in range(4):\n",
    "        \n",
    "        df = read_csv_as_df('test_portfolio_metrics'+str(i)+'.csv',folder_name)\n",
    "        \n",
    "        mean_values = df['Mean']\n",
    "        \n",
    "        std_values = df['Standard Deviation']\n",
    "        \n",
    "        for j in range(7):\n",
    "        \n",
    "            data.append(mean_values[j])\n",
    "            data.append(std_values[j])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608adc39",
   "metadata": {},
   "source": [
    "*Setting the initial Capital*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841f3617",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialCapital=1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4493ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['arima', 'arima_garch', 'transformer_mcdropout', 'transformer_garch', 'transformer_enbpi']\n",
    "\n",
    "alphas = [0.15, 0.1, 0.05, 0.01]\n",
    "\n",
    "optimises=['sharpe'] #Does not have use in the following code\n",
    "\n",
    "collated_data = list()\n",
    "collated_columns = ['folder_name', 'optimise', 'days', 'short', 's', 'Long -> Cumulative Returns -> Mean', 'Long -> Cumulative Returns -> Std', 'Long -> Annualized Volatility -> Mean', 'Long -> Annualized Volatility -> Std', 'Long -> Sharpe Ratio -> Mean', 'Long -> Sharpe Ratio -> Std', 'Long -> Sortino Ratio -> Mean', 'Long -> Sortino Ratio -> Std', 'Long -> Maximum Drawdown -> Mean', 'Long -> Maximum Drawdown -> Std', 'Long -> Beta -> Mean', 'Long -> Beta -> Std', 'Long -> Alpha -> Mean', 'Long -> Alpha -> Std', 'Long using thresholded certainty -> Cumulative Returns -> Mean', 'Long using thresholded certainty -> Cumulative Returns -> Std', 'Long using thresholded certainty -> Annualized Volatility -> Mean', 'Long using thresholded certainty -> Annualized Volatility -> Std', 'Long using thresholded certainty -> Sharpe Ratio -> Mean', 'Long using thresholded certainty -> Sharpe Ratio -> Std', 'Long using thresholded certainty -> Sortino Ratio -> Mean', 'Long using thresholded certainty -> Sortino Ratio -> Std', 'Long using thresholded certainty -> Maximum Drawdown -> Mean', 'Long using thresholded certainty -> Maximum Drawdown -> Std', 'Long using thresholded certainty -> Beta -> Mean', 'Long using thresholded certainty -> Beta -> Std', 'Long using thresholded certainty -> Alpha -> Mean', 'Long using thresholded certainty -> Alpha -> Std', 'Short -> Cumulative Returns -> Mean', 'Short -> Cumulative Returns -> Std', 'Short -> Annualized Volatility -> Mean', 'Short -> Annualized Volatility -> Std', 'Short -> Sharpe Ratio -> Mean', 'Short -> Sharpe Ratio -> Std', 'Short -> Sortino Ratio -> Mean', 'Short -> Sortino Ratio -> Std', 'Short -> Maximum Drawdown -> Mean', 'Short -> Maximum Drawdown -> Std', 'Short -> Beta -> Mean', 'Short -> Beta -> Std', 'Short -> Alpha -> Mean', 'Short -> Alpha -> Std', 'Short using thresholded certainty -> Cumulative Returns -> Mean', 'Short using thresholded certainty -> Cumulative Returns -> Std', 'Short using thresholded certainty -> Annualized Volatility -> Mean', 'Short using thresholded certainty -> Annualized Volatility -> Std', 'Short using thresholded certainty -> Sharpe Ratio -> Mean', 'Short using thresholded certainty -> Sharpe Ratio -> Std', 'Short using thresholded certainty -> Sortino Ratio -> Mean', 'Short using thresholded certainty -> Sortino Ratio -> Std', 'Short using thresholded certainty -> Maximum Drawdown -> Mean', 'Short using thresholded certainty -> Maximum Drawdown -> Std', 'Short using thresholded certainty -> Beta -> Mean', 'Short using thresholded certainty -> Beta -> Std', 'Short using thresholded certainty -> Alpha -> Mean', 'Short using thresholded certainty -> Alpha -> Std']#['folder_name', 'optimise', 'days', 'short', 's', 'Long -> Cumulative Returns -> Mean', 'Long -> Cumulative Returns -> Std', 'Long -> Annualized Volatility -> Mean', 'Long -> Annualized Volatility -> Std', 'Long -> Sharpe Ratio -> Mean', 'Long -> Sharpe Ratio -> Std', 'Long -> Sortino Ratio -> Mean', 'Long -> Sortino Ratio -> Std', 'Long -> Maximum Drawdown -> Mean', 'Long -> Maximum Drawdown -> Std', 'Long using thresholded certainty -> Cumulative Returns -> Mean', 'Long using thresholded certainty -> Cumulative Returns -> Std', 'Long using thresholded certainty -> Annualized Volatility -> Mean', 'Long using thresholded certainty -> Annualized Volatility -> Std', 'Long using thresholded certainty -> Sharpe Ratio -> Mean', 'Long using thresholded certainty -> Sharpe Ratio -> Std', 'Long using thresholded certainty -> Sortino Ratio -> Mean', 'Long using thresholded certainty -> Sortino Ratio -> Std', 'Long using thresholded certainty -> Maximum Drawdown -> Mean', 'Long using thresholded certainty -> Maximum Drawdown -> Std', 'Short -> Cumulative Returns -> Mean', 'Short -> Cumulative Returns -> Std', 'Short -> Annualized Volatility -> Mean', 'Short -> Annualized Volatility -> Std', 'Short -> Sharpe Ratio -> Mean', 'Short -> Sharpe Ratio -> Std', 'Short -> Sortino Ratio -> Mean', 'Short -> Sortino Ratio -> Std', 'Short -> Maximum Drawdown -> Mean', 'Short -> Maximum Drawdown -> Std', 'Short using thresholded certainty -> Cumulative Returns -> Mean', 'Short using thresholded certainty -> Cumulative Returns -> Std', 'Short using thresholded certainty -> Annualized Volatility -> Mean', 'Short using thresholded certainty -> Annualized Volatility -> Std', 'Short using thresholded certainty -> Sharpe Ratio -> Mean', 'Short using thresholded certainty -> Sharpe Ratio -> Std', 'Short using thresholded certainty -> Sortino Ratio -> Mean', 'Short using thresholded certainty -> Sortino Ratio -> Std', 'Short using thresholded certainty -> Maximum Drawdown -> Mean', 'Short using thresholded certainty -> Maximum Drawdown -> Std']#['folder_name', 'optimise','val_thres', 'test_thres', 'days', 'short', 's', 'optimised_value_on_val', 'optimised_value_on_test', 'test_percent_da', 'Long -> Cumulative Returns -> Mean', 'Long -> Cumulative Returns -> Std', 'Long -> Annualized Volatility -> Mean', 'Long -> Annualized Volatility -> Std', 'Long -> Sharpe Ratio -> Mean', 'Long -> Sharpe Ratio -> Std', 'Long -> Sortino Ratio -> Mean', 'Long -> Sortino Ratio -> Std', 'Long -> Maximum Drawdown -> Mean', 'Long -> Maximum Drawdown -> Std', 'Long using certainty -> Cumulative Returns -> Mean', 'Long using certainty -> Cumulative Returns -> Std', 'Long using certainty -> Annualized Volatility -> Mean', 'Long using certainty -> Annualized Volatility -> Std', 'Long using certainty -> Sharpe Ratio -> Mean', 'Long using certainty -> Sharpe Ratio -> Std', 'Long using certainty -> Sortino Ratio -> Mean', 'Long using certainty -> Sortino Ratio -> Std', 'Long using certainty -> Maximum Drawdown -> Mean', 'Long using certainty -> Maximum Drawdown -> Std', 'Long using thresholded certainty -> Cumulative Returns -> Mean', 'Long using thresholded certainty -> Cumulative Returns -> Std', 'Long using thresholded certainty -> Annualized Volatility -> Mean', 'Long using thresholded certainty -> Annualized Volatility -> Std', 'Long using thresholded certainty -> Sharpe Ratio -> Mean', 'Long using thresholded certainty -> Sharpe Ratio -> Std', 'Long using thresholded certainty -> Sortino Ratio -> Mean', 'Long using thresholded certainty -> Sortino Ratio -> Std', 'Long using thresholded certainty -> Maximum Drawdown -> Mean', 'Long using thresholded certainty -> Maximum Drawdown -> Std', 'Short -> Cumulative Returns -> Mean', 'Short -> Cumulative Returns -> Std', 'Short -> Annualized Volatility -> Mean', 'Short -> Annualized Volatility -> Std', 'Short -> Sharpe Ratio -> Mean', 'Short -> Sharpe Ratio -> Std', 'Short -> Sortino Ratio -> Mean', 'Short -> Sortino Ratio -> Std', 'Short -> Maximum Drawdown -> Mean', 'Short -> Maximum Drawdown -> Std', 'Short using certainty -> Cumulative Returns -> Mean', 'Short using certainty -> Cumulative Returns -> Std', 'Short using certainty -> Annualized Volatility -> Mean', 'Short using certainty -> Annualized Volatility -> Std', 'Short using certainty -> Sharpe Ratio -> Mean', 'Short using certainty -> Sharpe Ratio -> Std', 'Short using certainty -> Sortino Ratio -> Mean', 'Short using certainty -> Sortino Ratio -> Std', 'Short using certainty -> Maximum Drawdown -> Mean', 'Short using certainty -> Maximum Drawdown -> Std', 'Short using thresholded certainty -> Cumulative Returns -> Mean', 'Short using thresholded certainty -> Cumulative Returns -> Std', 'Short using thresholded certainty -> Annualized Volatility -> Mean', 'Short using thresholded certainty -> Annualized Volatility -> Std', 'Short using thresholded certainty -> Sharpe Ratio -> Mean', 'Short using thresholded certainty -> Sharpe Ratio -> Std', 'Short using thresholded certainty -> Sortino Ratio -> Mean', 'Short using thresholded certainty -> Sortino Ratio -> Std', 'Short using thresholded certainty -> Maximum Drawdown -> Mean', 'Short using thresholded certainty -> Maximum Drawdown -> Std']\n",
    "\n",
    "data_folders = ['\\model_data_1','\\model_data_2','\\model_data_3','\\model_data_5','\\model_data_6','\\model_data_OT']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee27b5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "for data_folder in data_folders:\n",
    "    \n",
    "    # Creating a folder where to store the trading data\n",
    "    main_folder = create_unique_folder('trading_'+data_folder[1:])\n",
    "    \n",
    "    for optimise in optimises:\n",
    "\n",
    "        for model in models:\n",
    "\n",
    "            if model == 'transformer_enbpi':\n",
    "                ss = [5, 10, 20, 25, 50, 100]\n",
    "            else:\n",
    "                ss = [None]\n",
    "\n",
    "            for s in ss:\n",
    "\n",
    "                if s != None:\n",
    "                    model_step = s*6\n",
    "                else:\n",
    "                    model_step = 30\n",
    "\n",
    "                for alpha in alphas:\n",
    "\n",
    "                    print(f\"Optimise: {optimise}, Model: {model}, s: {s}, Alpha: {alpha}, Data Folder: {data_folder}\")\n",
    "\n",
    "                    # Creating a folder for the results\n",
    "                    sub_folder_name = model+'_alpha'+str(alpha)+'_s'+str(s)+'_optimise'+optimise\n",
    "\n",
    "                    # Create the directory for the results\n",
    "                    folder_name = os.path.join(main_folder, sub_folder_name)\n",
    "                    os.makedirs(folder_name, exist_ok=True)\n",
    "                    \n",
    "                    # Loading data\n",
    "                    y_true, y_pred, y_pred_lower, y_pred_upper, val_y_true, val_y_pred, val_y_pred_lower, val_y_pred_upper, og_data_true, og_data_date = load_data(model=model, alpha=alpha, s=s, data_folder=data_folder)\n",
    "\n",
    "                    # Preparing the data\n",
    "                    index, tradedate, date, tradeactualPrice, actualPrice, val_index, val_tradedate, val_date, val_tradeactualPrice, val_actualPrice = prepare_data(y_true, val_y_true, og_data_date, og_data_true)\n",
    "\n",
    "                    # Classifying the predictions\n",
    "                    actualClass, predClass = convert_to_class(index, actualPrice, tradeactualPrice, y_pred)\n",
    "                    val_actualClass, val_predClass = convert_to_class(val_index, val_actualPrice, val_tradeactualPrice, val_y_pred)\n",
    "\n",
    "                    # Normalising the uncertainty\n",
    "                    norm_val_uncert, norm_test_uncert = normalise_test_uncert(val_y_pred_upper, val_y_pred_lower, y_pred_upper, y_pred_lower)\n",
    "\n",
    "                    # Formating the dated\n",
    "                    tradedate, date, val_tradedate, val_date = format_dates(tradedate, date, val_tradedate, val_date)\n",
    "\n",
    "                    # Creating a df that is acceptable by the trading algo\n",
    "                    df = create_df(index, tradedate, date, tradeactualPrice, actualPrice, actualClass, predClass, norm_test_uncert)\n",
    "                    val_df = create_df(val_index, val_tradedate, val_date, val_tradeactualPrice, val_actualPrice, val_actualClass, val_predClass, norm_val_uncert)\n",
    "\n",
    "                    # Training a model to get threshold and position size\n",
    "                    X_train, y_train = get_lin_reg_dataset(val_df, step=model_step)\n",
    "                    X_test, y_test = get_lin_reg_dataset(df, test=True, val_df=val_df, step=model_step)\n",
    "\n",
    "                    # Train the model\n",
    "                    model_tuner = train_model(X_train, y_train)\n",
    "\n",
    "                    # Test the model\n",
    "                    y_proba, accuracy = test_model_with_threshold(model_tuner, X_test, y_test)\n",
    "\n",
    "                    df['certainty'] = np.where(y_proba < 0.5, 0, y_proba)\n",
    "                    \n",
    "                    if model == 'transformer_enbpi' or model == 'transformer_mcdropout':\n",
    "                        \n",
    "                        # Define the path to your CSV file\n",
    "                        csv_file_path = r'C:\\Users\\porte\\Downloads' + data_folder + r'\\preds_run_0_ns_Transformer_1_' + data_folder.split('_')[-1] + '.csv'\n",
    "\n",
    "                        # Initialize an empty list to store the numerical data\n",
    "                        og_pred_data = []\n",
    "\n",
    "                        # Open and read the CSV file\n",
    "                        with open(csv_file_path, newline='') as csvfile:\n",
    "                            reader = csv.reader(csvfile)\n",
    "                            for row in reader:\n",
    "                                # Convert each element in the row to float and extend the numerical_data list\n",
    "                                og_pred_data.extend([float(item) for item in row])\n",
    "\n",
    "                        # replicate df\n",
    "                        df_og_data = df.copy()\n",
    "                        \n",
    "                        # replace the predClass\n",
    "                        actualClass_NotUSED, new_predClass = convert_to_class(index, actualPrice, tradeactualPrice, og_pred_data)\n",
    "                        df_og_data['predClass'] = og_pred_data\n",
    "                        \n",
    "                        trade_on_test(df, initialCapital, folder_name, df_og_data=df_og_data)\n",
    "                    \n",
    "                    else:\n",
    "                        trade_on_test(df, initialCapital, folder_name, df_og_data=None)\n",
    "                        \n",
    "                    # Tuning the threshold on the valoidation data\n",
    "                    ##tuned_threshold = tune_threshold(val_df, initialCapital, folder_name, useCertainty=True, short=short, optimise=optimise, days=days)\n",
    "\n",
    "                    # Tuning on the test data to check how different the threshold is\n",
    "                    # Tuning the threshold on the valoidation data\n",
    "                    ##tuned_threshold_test = tune_threshold(df, initialCapital, folder_name, useCertainty=True, short=short, optimise=optimise, days=days, test=True)\n",
    "\n",
    "                    # Plotting the superimpose thresholded parameters\n",
    "                    ##superimpose_tuned_data(folder_name, short, days, optimise)\n",
    "\n",
    "                    # Trading on the test data using the threshold obtained from the validation data\n",
    "                    #trade_on_test(df, tuned_threshold, initialCapital, folder_name)\n",
    "                    #trade_on_test(df, initialCapital, folder_name)\n",
    "\n",
    "                    # Plotting the equity curve for all the different methods\n",
    "                    plot_equity_curve(df, initialCapital, folder_name)\n",
    "\n",
    "                    collated_data.append(collate_data(folder_name, optimise, days, short, s))\n",
    "\n",
    "\n",
    "    collated_data_df = pd.DataFrame(collated_data, columns=collated_columns)\n",
    "    collated_data_df.to_csv('collated_data_df_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0c30be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413d02ce",
   "metadata": {},
   "source": [
    "*Checking folder paths are correct*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52173b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for data_folder in data_folders:\n",
    "    \n",
    "    # Creating a folder where to store the trading data\n",
    "    main_folder = create_unique_folder('trading_'+data_folder[1:])\n",
    "    \n",
    "    for optimise in optimises:\n",
    "\n",
    "        for model in models:\n",
    "\n",
    "            if model == 'transformer_enbpi':\n",
    "                ss = [5, 10, 20, 25, 50, 100]\n",
    "            else:\n",
    "                ss = [None]\n",
    "\n",
    "            for s in ss:\n",
    "\n",
    "                if s != None:\n",
    "                    model_step = s*6\n",
    "                else:\n",
    "                    model_step = 30\n",
    "\n",
    "                for alpha in alphas:\n",
    "\n",
    "                    print(f\"Optimise: {optimise}, Model: {model}, s: {s}, Alpha: {alpha}, Data Folder: {data_folder}\")\n",
    "\n",
    "                    # Creating a folder for the results\n",
    "                    sub_folder_name = model+'_alpha'+str(alpha)+'_s'+str(s)+'_optimise'+optimise\n",
    "\n",
    "                    # Create the directory for the results\n",
    "                    folder_name = os.path.join(main_folder, sub_folder_name)\n",
    "                    os.makedirs(folder_name, exist_ok=True)\n",
    "                    \n",
    "                    # Loading data\n",
    "                    y_true, y_pred, y_pred_lower, y_pred_upper, val_y_true, val_y_pred, val_y_pred_lower, val_y_pred_upper, og_data_true, og_data_date = load_data(model=model, alpha=alpha, s=s, data_folder=data_folder)\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6c4bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
